{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "MocJN22HSJ1x",
        "iWlfabmkaSE7",
        "Pu_lXic2aSHd",
        "rVZvHtwpaSHq",
        "Fz39wa78wGYR",
        "8xlq48WjiW6U",
        "PYNcwKnAz5Hf",
        "LZEWJXrNaSIf"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0tyIsliieNr"
      },
      "source": [
        "# **Competencia 2 - CC6205 Natural Language Processing 📚**\n",
        "\n",
        "Integrantes: Sebastián Valdivia - Patricio Ortiz - Marcelo Fuentealba\n",
        "\n",
        "Fecha límite de entrega 📆: 29 de Junio.\n",
        "\n",
        "Link competencia: https://codalab.lisn.upsaclay.fr/competitions/5098?secret_key=09955d45-6210-4a35-a171-8050aa050855#results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MocJN22HSJ1x"
      },
      "source": [
        "### **Objetivo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwdgXS8FSLvc"
      },
      "source": [
        "El objetivo de esta competencia es resolver una de las tareas más importantes en el área del procesamiento de lenguage natural, relacionada con la extracción de información: [Named Entity Recognition (NER)](http://www.cs.columbia.edu/~mcollins/cs4705-spring2019/slides/tagging.pdf). \n",
        "\n",
        "En particular, y al igual que en la competencia anterior, deberán crear distintos modelos que apunten a resolver la tarea de NER en Español. Para esto, les entregaremos un dataset real perteneciente a la lista de espera NO GES en Chile. Es importante destacar que existe una falta de trabajos realizados en el área de NER en Español y aún más en el contexto clínico, por ende puede ser considerado como una tarea bien desafiante y quizás les interesa trabajar en el área más adelante en sus carreras.\n",
        "\n",
        "En este notebook les entregaremos un baseline como referencia de los resultados que esperamos puedan obtener. Recuerden que el no superar a los baselines en alguna de las tres métricas conlleva un descuento de 0.5 puntos hasta 1.5 puntos.\n",
        "\n",
        "Como hemos estado viendo redes neuronales tanto en catedras, tareas y auxiliares (o próximamente lo harán), esperamos que (por lo menos) utilicen Redes Neuronales Recurrentes (RNN) para resolverla. \n",
        "\n",
        "Nuevamente, hay total libertad para utilizar el software y los modelos que deseen, siempre y cuando estos no traigan los modelos ya implementados. (De todas maneras como es un corpus nuevo, es difícil que haya algún modelo ya implementado con estas entidades)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnjgmvjBSReb"
      },
      "source": [
        "### **Explicación de la competencia**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NH4HqnCjSWs-"
      },
      "source": [
        "La tarea **NER** que van a resolver en esta competencia es comúnmente abordada como un problema de Sequence Labeling.\n",
        "\n",
        "**¿Qué es Sequence Labeling?** \n",
        "\n",
        "En breves palabras, dada una secuencia de tokens (oración) sequence labeling tiene por objetivo asignar una etiqueta a cada token de dicha secuencia. En pocas palabras, dada una lista de tokens esperamos encontrar la mejor secuencia de etiquetas asociadas a esa lista. Ahora veamos de qué se trata este problema.\n",
        "\n",
        "**Named Entity Recognition (NER)**\n",
        "\n",
        "NER es un ejemplo de un problema de Sequence Labeling. Pero antes de definir formalmente esta tarea, es necesario definir algunos conceptos claves para poder entenderla de la mejor manera:\n",
        "\n",
        "- *Token*: Un token es una secuencia de caracteres, puede ser una palabra, un número o un símbolo.\n",
        "\n",
        "- *Entidad*: No es más que un trozo de texto (uno o más tokens) asociado a una categoría predefinida. Originalmente se solían utilizar categorías como nombres de personas, organizaciones, ubicaciones, pero actualmente se ha extendido a diferentes dominios.\n",
        "\n",
        "- *Límites de una entidad*: Son los índices de los tokens de inicio y fín dentro de una entidad.\n",
        "\n",
        "- *Tipo de entidad*: Es la categoría predefinida asociada a la entidad.\n",
        "\n",
        "Dicho esto, definimos formalmente una entidad como una tupla: $(s, e, t)$, donde $s, t$ son los límites de la entidad (índices de los tokens de inicio y fin, respectivamente) y t corresponde al tipo de entidad o categoría. Ya veremos más ejemplos luego de describir el Dataset.\n",
        "\n",
        "**Corpus de la Lista de espera**\n",
        "\n",
        "Trabajaran con un conjunto de datos reales correspondiente a interconsultas de la lista de espera NO GES en Chile. Si quieren saber más sobre cómo fueron generados los datos pueden revisar el paper publicado hace unos meses atrás en el workshop de EMNLP, una de las conferencias más importantes de NLP: [https://www.aclweb.org/anthology/2020.clinicalnlp-1.32/](https://www.aclweb.org/anthology/2020.clinicalnlp-1.32/).\n",
        "\n",
        "Este corpus Chileno está constituido originalmente por 7 tipos de entidades pero por simplicidad en esta competencia trabajarán con las siguientes:\n",
        "\n",
        "- **Disease**\n",
        "- **Body_Part**\n",
        "- **Medication** \n",
        "- **Procedures** \n",
        "- **Family_Member**\n",
        "\n",
        "Si quieren obtener más información sobre estas entidades pueden consultar la [guía de anotación](https://plncmm.github.io/annodoc/). Además, mencionar que este corpus está restringido bajo una licencia que permite solamente su uso académico, así que no puede ser compartido más allá de este curso o sin permisos por parte de los autores en caso que quieran utilizarlo fuera. Si este último es el caso entonces pueden escribir directamente al correo: pln@cmm.uchile.cl. Al aceptar los términos y condiciones de la competencia están de acuerdo con los puntos descritos anteriormente.\n",
        "\n",
        "\n",
        "**Formato ConLL**\n",
        "\n",
        "Los archivos que serán entregados a ustedes vienen en un formato estándar utilizado en NER, llamado ConLL. No es más que un archivo de texto, que cumple las siguientes propiedades.\n",
        "\n",
        "- Un salto de linea corresponde a la separación entre oraciones. Esto es importante ya que al entrenar una red neuronal ustedes pasaran una lista de oraciones como input, más conocidos como batches.\n",
        "\n",
        "- La primera columna del archivo contiene todos los tokens de la partición.\n",
        "\n",
        "- La segunda columna del archivo contiene el tipo de entidad asociado al token de la primera columna.\n",
        "\n",
        "- Los tipos de entidades siguen un formato clásico en NER denominado *IOB2*. Si un tipo de entidad comienza con el prefijo \"B-\" (Beginning) significa que es el token de inicio de una entidad, si comienza con \"I-\" (Inside) es un token distinto al de inicio y si un token está asociado a la categoría O (Outside) significa que no pertenece a ninguna entidad.\n",
        "\n",
        "Aquí va un ejemplo:\n",
        "\n",
        "```\n",
        "PACIENTE O\n",
        "PRESENTA O\n",
        "FRACTURA B-Disease\n",
        "CORONARIA I-Disease\n",
        "COMPLICADA I-Disease\n",
        "EN O\n",
        "PIE B-Body_Part\n",
        "IZQUIERDO I-Body_Part\n",
        ". O\n",
        "SE O\n",
        "REALIZA O\n",
        "INSTRUMENTACION B-Procedure\n",
        "INTRACONDUCTO I-Procedure\n",
        ". O\n",
        "```\n",
        "\n",
        "Según nuestra definición tenemos las siguientes tres entidades (enumerando desde 0): \n",
        "\n",
        "- $(2, 4, Disease)$\n",
        "- $(6, 7, Body Part)$\n",
        "- $(11, 12, Procedure)$\n",
        "\n",
        "Repasen un par de veces todos estos conceptos antes de pasar a la siguiente sección del notebook.\n",
        "Es importante entender bien este formato ya que al medir el rendimiento de sus modelos, consideraremos una **métrica estricta**. Esta métrica se llama así ya que considera correcta una predicción de su modelo, sólo si al compararlo con las entidades reales **coinciden tanto los límites de la entidad como el tipo.** \n",
        "\n",
        "Para ejemplificar, tomando el caso anterior, si el modelo es capaz de encontrar la siguiente entidad: $(2, 3, Disease)$, entonces se considera incorrecto ya que pudo predecir dos de los tres tokens de dicha enfermedad. Es decir, buscamos una métrica que sea alta a nivel de entidad y no a nivel de token.\n",
        "\n",
        "Antes de pasar a explicar las reglas, se recomienda visitar los siguientes links para entender bien el baseline de la competencia:\n",
        "\n",
        "-  [Tagging, and Hidden Markov Models ](http://www.cs.columbia.edu/~mcollins/cs4705-spring2019/slides/tagging.pdf) (slides by Michael Collins), [notes](http://www.cs.columbia.edu/~mcollins/hmms-spring2013.pdf), [video 1](https://youtu.be/-ngfOZz8yK0), [video 2](https://youtu.be/PLoLKQwkONw), [video 3](https://youtu.be/aaa5Qoi8Vco), [video 4](https://youtu.be/4pKWIDkF_6Y)       \n",
        "-  [Recurrent Neural Networks](slides/NLP-RNN.pdf) | [video 1](https://youtu.be/BmhjUkzz3nk), [video 2](https://youtu.be/z43YFR1iIvk), [video 3](https://youtu.be/7L5JxQdwNJk)\n",
        "\n",
        "\n",
        "Recuerden que todo el material se encuentra disponible en el [github del curso](https://github.com/dccuchile/CC6205)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWlfabmkaSE7"
      },
      "source": [
        "### **Reglas de la competencia**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w9Dw4CSaSE8"
      },
      "source": [
        "- Para que su competencia sea evaluada, deben participar en la competencia y enviar este notebook con su informe.\n",
        "- Para participar, deben registrarse en la competencia en Codalab en grupos de máximo 3 alumnos. Cada grupo debe tener un nombre de equipo. (¡Y deben reportarlo en su informe, por favor!)\n",
        "- Las métricas usadas serán métricas estrictas (ya explicado anteriormente) utilizando métricas clásicas como lo son precisión, recall y micro f1-score.\n",
        "- En esta tarea se recomienda usar GPU. Pueden ejecutar su tarea en colab (lo cual trae todo instalado) o pueden intentar ejecutándolo en su computador. En este caso, deberá ser compatible con cuda y deberán instalar todo por su cuenta.\n",
        "- En total pueden hacer un **máximo de 5 envíos**.\n",
        "- Por favor, todas sus dudas haganlas por el canal de Discord. Los emails que lleguen al equipo docente serán remitidos a ese medio. Recuerden el ánimo colaborativo del curso.\n",
        "- Estar top 5 en alguna de las tres métricas equivale a una bonificación en su nota final.\n",
        "\n",
        "Éxito!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZyHBjU-R-wi"
      },
      "source": [
        "### **Baseline**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WZ8G01aSBYX"
      },
      "source": [
        "En este punto esperamos que tengan conocimiento sobre redes neuronales y en particular redes neuronales recurrentes (RNN), si no siempre pueden escribirnos por el canal de Discord para aclarar dudas. La RNN del baseline adjunto a este notebook está programado en la librería [`pytorch`](https://pytorch.org/) pero ustedes pueden utilizar keras, tensorflow si así lo desean. El código contiene lo siguiente:\n",
        "\n",
        "- La carga de los datasets, creación de batches de texto y padding (esto es importante ya que si utilizan redes neuronales tienen que tener el mismo largo los inputs). \n",
        "\n",
        "- La implementación básica de una red `LSTM` simple de solo un nivel y sin bi-direccionalidad. \n",
        "\n",
        "- La construcción del formato del output requerido para que lo puedan probar en la tarea en codalab.\n",
        "\n",
        "Se espera que como mínimo ustedes puedan experimentar con el baseline utilizando (pero no limitándose) estas sugerencias:\n",
        "\n",
        "*   Probar la técnica de early stopping.\n",
        "*   Variar la cantidad de parámetros de la capa de embeddings.\n",
        "*   Variar la cantidad de capas RNN.\n",
        "*   Variar la cantidad de parámetros de las capas de RNN.\n",
        "*   Inicializar la capa de embeddings con modelos pre-entrenados. (word2vec, glove, conceptnet, etc...). [Embeddings en español aquí](https://github.com/dccuchile/spanish-word-embeddings). También aquí pueden encontrar unos embeddings clínicos en Español: [https://zenodo.org/record/3924799](https://zenodo.org/record/3924799)\n",
        "*   Variar la cantidad de épocas de entrenamiento.\n",
        "*   Variar el optimizador, learning rate, batch size, usar CRF loss, etc.\n",
        "*   Probar una capa de CRF para garantizar el     formato IOB2.\n",
        "*   Probar bi-direccionalidad.\n",
        "*   Incluir dropout.\n",
        "*   Probar modelos de tipo GRU.\n",
        "*   Probar usando capas de atención.\n",
        "*   Probar Embedding Contextuales (les puede ser de utilidad [flair](https://github.com/flairNLP/flair))\n",
        "*   Probar modelos de transformers en español usando [Huggingface](https://github.com/huggingface/transformers) o el framework Flair."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Rr2mzxPTzNd"
      },
      "source": [
        "### **Reporte**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEf33mnxT0rf"
      },
      "source": [
        "Este debe cumplir la siguiente estructura:\n",
        "\n",
        "1.\t**Introducción**: Presentar brevemente el contexto, problema a resolver, incluyendo la formalización de la task (cómo son los inputs y outputs del problema) y los desafíos que ven al analizar el corpus entregado. (**0.5 puntos**)\n",
        "\n",
        "2.\t**Modelos**: Describir brevemente los modelos, métodos e hiperparámetros utilizados. (**1.0 puntos**)\n",
        "\n",
        "4.\t**Métricas de evaluación**: Describir las métricas utilizadas en la evaluación indicando qué miden y cuál es su interpretación en este problema en particular. (**0.5 puntos**)\n",
        "\n",
        "5.  **Diseño experimental**: Esta es una de las secciones más importantes del reporte. Deben describir minuciosamente los experimentos que realizarán en la siguiente sección. Describir las variables de control que manejarán, algunos ejemplos pueden ser: Los hiperparámetros de los modelos, tipo de embeddings utilizados, tipos de arquitecturas. Ser claros con el conjunto de hiperparámetros que probarán, la decisión en las funciones de optimización, función de pérdida,  regulación, etc. Básicamente explicar qué es lo que veremos en la siguiente sección.\n",
        "(**1 punto**)\n",
        "\n",
        "6.\t**Experimentos**: Reportar todos sus experimentos y código en esta sección. Comparar los resultados obtenidos utilizando diferentes modelos. ¡Es vital haber realizado varios experimentos para sacar una buena nota! (**2.0 puntos**)\n",
        "\n",
        "7.\t**Conclusiones**: Discutir resultados, proponer trabajo futuro. (**1 punto**)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaoU1EXfUDbl"
      },
      "source": [
        "# **Entregable.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzQlYlmGaSFH"
      },
      "source": [
        "## **Introducción**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVL0-01AUOzL"
      },
      "source": [
        "Esta competencia se da en el contexto de las listas de espera NO GES en Chile, en particular, se debe resolver la tarea de Named Entity Recognition (NER) para un dataset que contiene el texto anotado en fichas clínicas de interconsultas de pacientes. Las categorías de entidad que se disponen para etiquetar corresponden a:\n",
        "* Disease: Enfermedades mapeables a un código CIE-10.\n",
        "* Body_Part: Órgano o parte anatómica de una persona.\n",
        "* Medication: Todas las menciones de medicamentos o drogas empleadas.\n",
        "* Procedures: Procedimientos diagnósticos, terapéuticos y de laboratorio.\n",
        "* Family_Member: Etiqueta para miembros de la familia.\n",
        "\n",
        "Los inputs están en formato ConLL, los cuales deben ser procesados y adaptados para hacer una adecuada lectura de ellos. Visto de manera simple, un input corresponde a una oración que contiene entidades.\n",
        "El output del problema consiste en un vector que contiene la etiqueta de las entidades correspondientes al input entregado.\n",
        "\n",
        "Los desafíos que se enfrentan en la task se centran principalmente montar la infraestructura necesaria para que los modelos sean capaces de mantener \"memoria\" y de esta manera lograr hacer un buen NER.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbA1EmhCaSFI"
      },
      "source": [
        "## **Modelos**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HsvlfPJUSId"
      },
      "source": [
        "Para abordar este trabajo se utilizan 3 modelos de redes neuronales y finalmente a cada uno se le aplica bidireccionalidad.\n",
        "\n",
        "En primer lugar, se trabaja con LSTM que corresponden a un tipo específico de redes neuronales recurrentes. Se caracterizan por tener la capacidad de \"recordar\" estados previos y utilizar dicha información para definir el próximo estado. Para el caso específico de NER son útiles porque pueden aprender dependencias y no solo de corto plazo, de modo que se evita el supuesto de independencia que se hace, por ejemplo, en los modelos HMM.\n",
        "MÉTODOS, HIPERPARÁMETROS\n",
        "Para este caso, los hiperparámetros corresponden a la dimensión de los embeddings, dimensión de las capas LSTM, el número de capas, dropout, número de épocas.\n",
        "\n",
        "\n",
        "En segundo lugar, se trabaja con redes Elman, caracterizadas por generar y detectar patrones variantes, y también es capaz de retener información de estados previos.\n",
        "MÉTODOS, HIPERPARÁMETROS\n",
        "En este caso, los hiperparámetros son la dimensión de los embeddings, dimensión de las capas LSTM, el número de capas, dropout.\n",
        "\n",
        "En tercer lugar, se utilizan redes tipo GRU que corresponde a una variante y simplificación de la LSTM. GRU también puede resolver el problema de dependencia de largo plazo.\n",
        "MÉTODOS, HIPERPARÁMETROS\n",
        "En este caso, los hiperparámetros son la dimensión de los embeddings, dimensión de las capas LSTM, el número de capas, dropout.\n",
        "\n",
        "Por último, se toma cada uno de los modelos y se implementa bidireccionalidad, con el fin de incorporar la capacidad de acceder a contextos futuros y no solo pasados, aspecto que es candidato a mejorar el rendimiento de los modelos en la task. Para este caso, los hiperparámetros se mantienen según sea el modelo y se específica adicionalmente que existe bidireccionalidad."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaVhZ5iaaSFK"
      },
      "source": [
        "## **Métricas de evaluación**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXl3GaVMUYA7"
      },
      "source": [
        "- **Métrica estricta:** Cuando se habla de métrica estricta se considera que un documento está bien clasificado si cada una de las etiquetas predichas por el modelo son correctas, en caso contrario (al menos una etiqueta errónea), se considera mala clasificación.\n",
        "\n",
        "- **Precision:** Indica qué proporción de los documentos clasificados como positivos, por el modelo, son realmente son positivos. Responde a la pregunta ¿qué porcentaje de los que hemos dicho que son de clase positiva, en realidad lo son? (TP/(TP+FP))\n",
        "\n",
        " \n",
        "\n",
        "- **Recall:** Indica qué proporción de los documentos positivos fueron clasificados como positivos por el modelo. Responde a la pregunta ¿qué porcentaje de la clase positiva hemos sido capaces de identificar correctamente? (TP/(TP+FN))\n",
        "\n",
        "\n",
        "- **Micro F1 score:**\n",
        "PENDIENTE Recuerde hacer la distinción entre lo que sería una métrica de micro f1-score vs macro f1-score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u27WffRVUj4v"
      },
      "source": [
        "## **Diseño experimental**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O228DbeUmE7"
      },
      "source": [
        "###Early Stopping\n",
        "Para la primera sección de experimentos se implementa Early Stopping, ya que con esta técnica es posible experimentar sobre la mayor cantidad de épocas sobre los modelos sin tener que preocuparnos por el overfitting.\n",
        "Sin un mecanismo como este, es posible que la red termine sobreajustándose a los datos de entrenamiento si la sesión de entrenamiento no se detiene en el instante correcto, de modo que la red adquiere ruido y no se hace generalizable."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Redes Elman y GRU\n",
        "Para la siguiente experimentación se barajan alternativas a las redes recurentes LSTM que venía como baseline del proyecto, por lo que se procede a experimentar con modelos GRU y Elmann unidireccionales, ya que estas ya se encuentran implementadas en Pytorch, además de utilizar la función earlyStop_model para evitar el overfitting."
      ],
      "metadata": {
        "id": "MP0caBTw0htq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Redes Bidireccionales\n",
        "En este caso se tomarán las redes previamente trabajadas y se implementará la bidireccionalidad, aspecto interesante, ya que se trata de la combinación de dos redes: una que va de izquierda a derecha y otra que va de derecha a izquierda. Con esto es posible capturar el contexto de las oraciones considerando lo previo y lo posterior. Para NER, sabemos que el contexto cubre tags pasadas y futuras en una secuencia, por ello el hecho tener en cuenta tanto la información pasada como la futura podría agregar valor y mejorar el rendimiento de los modelos."
      ],
      "metadata": {
        "id": "qmySxRNj1lso"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFM-wNt8aSFM"
      },
      "source": [
        "## **Experimentos**\n",
        "\n",
        "\n",
        "El código que les entregaremos servirá de baseline para luego implementar mejores modelos. \n",
        "En general, el código asociado a la carga de los datos, las funciones de entrenamiento, de evaluación y la predicción de los datos de la competencia no deberían cambiar. \n",
        "Solo deben preocuparse de cambiar la arquitectura del modelo, sus hiperparámetros y reportar, lo cual lo pueden hacer en las subsecciones *modelos*.\n",
        "\n",
        "Los modelos con los que se experimentará serán:\n",
        "\n",
        "1. Modelos Elman y GRU: Se implementa versión unidireccional, con Early Stopping. Los hiperparametros usados son: Embedding de dimensión 300. Capas de LSTM de tamaño 256. Numero de capas = 3. Dropout de 0.3\n",
        "\n",
        "2. Modelos LSTM, Elman y GRU: Se implementa versión bidireccional, con Early Stopping. Los hiperparametros usados son: Embedding de dimensión 300. Capas de LSTM de tamaño 256. Numero de capas = 3. Dropout de 0.3\n",
        "\n",
        "3. Arquitecturas de LSTM bidireccional: Posterior a los ejercicios previos, se probaran distintas arquitecturas de LSTM's variando los hiperparametros.\n",
        "\n",
        "Para todos los modelos se utiliza función de pérdida CrossEntropy y optimizador Adam.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMgKjfYC_Go-"
      },
      "source": [
        "###  **Carga de datos y Preprocesamiento**\n",
        "\n",
        "Para cargar los datos y preprocesarlos usaremos la librería [`torchtext`](https://github.com/pytorch/text). Tener cuidado ya que hace algunos meses esta librería tuvo cambios radicales, quedando las funcionalidades pasadas en un nuevo paquete llamado legacy. Esto ya que si quieren usar más funciones de la librería entonces vean los cambios en la documentación.\n",
        "\n",
        "En particular usaremos su módulo `data`, el cual según su documentación original provee: \n",
        "\n",
        "    - Ability to describe declaratively how to load a custom NLP dataset that's in a \"normal\" format\n",
        "    - Ability to define a preprocessing pipeline\n",
        "    - Batching, padding, and numericalizing (including building a vocabulary object)\n",
        "    - Wrapper for dataset splits (train, validation, test)\n",
        "\n",
        "\n",
        "El proceso será el siguiente: \n",
        "\n",
        "1. Descargar los datos desde github y examinarlos.\n",
        "2. Definir los campos (`fields`) que cargaremos desde los archivos.\n",
        "3. Cargar los datasets.\n",
        "4. Crear el vocabulario."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27csY87GaSFO",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bde3a77b-0969-4847-d0c8-22b2f61e15a2"
      },
      "source": [
        "# Instalamos torchtext que nos facilitará la vida en el pre-procesamiento del formato ConLL.\n",
        "!pip install -U torchtext==0.10.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchtext==0.10.0 in /usr/local/lib/python3.7/dist-packages (0.10.0)\n",
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (1.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchtext==0.10.0) (4.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ng7wRGEyawjM"
      },
      "source": [
        "import torch\n",
        "from torchtext import data, datasets, legacy\n",
        "\n",
        "\n",
        "# Garantizar reproducibilidad de los experimentos\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BehSou6rCvwg"
      },
      "source": [
        "#### **Obtener datos**\n",
        "\n",
        "Descargamos los datos de entrenamiento, validación y prueba en nuestro directorio de trabajo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbT0g_kC18Jb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79add2cf-23ed-40b4-f9ec-c7abb26da7cc"
      },
      "source": [
        "#%%capture\n",
        "\n",
        "!wget https://github.com/dccuchile/CC6205/releases/download/v1.0/train.txt -nc # Dataset de Entrenamiento\n",
        "!wget https://github.com/dccuchile/CC6205/releases/download/v1.0/dev.txt -nc    # Dataset de Validación (Para probar y ajustar el modelo)\n",
        "!wget https://github.com/dccuchile/CC6205/releases/download/v1.0/test.txt -nc  # Dataset de la Competencia. Estos datos solo contienen los tokens. ¡¡SON LOS QUE DEBEN SER PREDICHOS!!"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‘train.txt’ already there; not retrieving.\n",
            "\n",
            "File ‘dev.txt’ already there; not retrieving.\n",
            "\n",
            "File ‘test.txt’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMud7YGMBZvg"
      },
      "source": [
        "####  **Fields**\n",
        "\n",
        "Un `field`:\n",
        "\n",
        "* Define un tipo de datos junto con instrucciones para convertir el texto a Tensor.\n",
        "* Contiene un objeto `Vocab` que contiene el vocabulario (palabras posibles que puede tomar ese campo).\n",
        "* Contiene otros parámetros relacionados con la forma en que se debe numericalizar un tipo de datos, como un método de tokenización y el tipo de Tensor que se debe producir.\n",
        "\n",
        "\n",
        "Analizemos el siguiente cuadro el cual contiene un ejemplo cualquiera de entrenamiento:\n",
        "\n",
        "\n",
        "```\n",
        "El O\n",
        "paciente O\n",
        "padece O\n",
        "de O\n",
        "cancer B-Disease\n",
        "de I-Disease\n",
        "colon I-Disease\n",
        ". O\n",
        "```\n",
        "\n",
        "Cada linea contiene un token y el tipo de entidad asociado en el formato IOB2 ya explicado. Para que `torchtext` pueda cargar estos datos, debemos definir como va a leer y separar los componentes de cada una de las lineas.\n",
        "Para esto, definiremos un field para cada uno de esos componentes: Las palabras (`TEXT`) y las etiquetas o categorías (`NER_TAGS`).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DcM_IjgCdzz"
      },
      "source": [
        "# Primer Field: TEXT. Representan los tokens de la secuencia\n",
        "TEXT = legacy.data.Field(lower=False) \n",
        "\n",
        "# Segundo Field: NER_TAGS. Representan los Tags asociados a cada palabra.\n",
        "NER_TAGS = legacy.data.Field(unk_token=None)\n",
        "fields = ((\"text\", TEXT), (\"nertags\", NER_TAGS))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fields"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBhoOQ64NG0L",
        "outputId": "8e21e1dd-a805-4e12-ce33-79609a344168"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('text', <torchtext.legacy.data.field.Field at 0x7fcf91cf9450>),\n",
              " ('nertags', <torchtext.legacy.data.field.Field at 0x7fcfef8bebd0>))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCKTJOdgC5eC"
      },
      "source": [
        "####  **SequenceTaggingDataset**\n",
        "\n",
        "`SequenceTaggingDataset` es una clase de torchtext diseñada para contener datasets de sequence labeling. Los ejemplos que se guarden en una instancia de estos serán arreglos de palabras asociados con sus respectivos tags.\n",
        "\n",
        "Por ejemplo, para Part-of-speech tagging:\n",
        "\n",
        "[I, love, PyTorch, .] estará asociado con [PRON, VERB, PROPN, PUNCT]\n",
        "\n",
        "\n",
        "La idea es que usando los fields que definimos antes, le indiquemos a la clase cómo cargar los datasets de prueba, validación y test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsHdGml62J21"
      },
      "source": [
        "train_data, valid_data, test_data = legacy.datasets.SequenceTaggingDataset.splits(\n",
        "    path=\"./\",\n",
        "    train=\"train.txt\",\n",
        "    validation=\"dev.txt\",\n",
        "    test=\"test.txt\",\n",
        "    fields=fields,\n",
        "    encoding=\"utf-8\",\n",
        "    separator=\" \"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hu7q3HCliia5",
        "outputId": "71235595-6d3a-4cf7-daad-b0fabc284201"
      },
      "source": [
        "print(f\"Numero de ejemplos de entrenamiento: {len(train_data)}\")\n",
        "print(f\"Número de ejemplos de validación: {len(valid_data)}\")\n",
        "print(f\"Número de ejemplos de test (competencia): {len(test_data)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numero de ejemplos de entrenamiento: 8025\n",
            "Número de ejemplos de validación: 891\n",
            "Número de ejemplos de test (competencia): 992\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDRnhXAdFGL-"
      },
      "source": [
        "Visualizemos un ejemplo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T023Ld4RaSF4",
        "scrolled": false,
        "outputId": "e43d929c-b527-4733-f404-55124f07b7e7"
      },
      "source": [
        "import random\n",
        "random_item_idx = random.randint(0, len(train_data))\n",
        "random_example = train_data.examples[random_item_idx]\n",
        "list(zip(random_example.text, random_example.nertags))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('-', 'O'),\n",
              " ('DESDENTADO', 'B-Disease'),\n",
              " ('PARCIAL', 'I-Disease'),\n",
              " ('/', 'O'),\n",
              " ('-', 'O'),\n",
              " ('Fundamento', 'O'),\n",
              " ('Clínico', 'O'),\n",
              " ('APS', 'O'),\n",
              " (':', 'O'),\n",
              " ('Paciente', 'O'),\n",
              " ('desdentado', 'B-Disease'),\n",
              " ('parcial', 'I-Disease'),\n",
              " ('superior', 'I-Disease'),\n",
              " ('e', 'I-Disease'),\n",
              " ('inferior', 'I-Disease'),\n",
              " (',', 'O'),\n",
              " ('se', 'O'),\n",
              " ('solicita', 'O'),\n",
              " ('protesis', 'B-Procedure'),\n",
              " ('removible', 'I-Procedure'),\n",
              " ('superior', 'I-Procedure'),\n",
              " ('e', 'I-Procedure'),\n",
              " ('inferior', 'I-Procedure'),\n",
              " ('.', 'O')]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2l05KYy5FSUy"
      },
      "source": [
        "#### **Construir los vocabularios para el texto y las etiquetas**\n",
        "\n",
        "Los vocabularios son los objetos que contienen todos los tokens (de entrenamiento) posibles para ambos fields. El siguiente paso consiste en construirlos. Para esto, hacemos uso del método `Field.build_vocab` sobre cada uno de nuestros `fields`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBhp7WICiibL"
      },
      "source": [
        "TEXT.build_vocab(train_data)\n",
        "NER_TAGS.build_vocab(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4OgUKM_iibO",
        "scrolled": true,
        "outputId": "d469ac2f-d382-4d6b-8818-b4affe3487ca"
      },
      "source": [
        "print(f\"Tokens únicos en TEXT: {len(TEXT.vocab)}\")\n",
        "print(f\"Tokens únicos en NER_TAGS: {len(NER_TAGS.vocab)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens únicos en TEXT: 17591\n",
            "Tokens únicos en NER_TAGS: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4FeyL9nFnId",
        "outputId": "28249759-90d5-49cb-a1ff-7075efa41c85"
      },
      "source": [
        "#Veamos las posibles etiquetas que hemos cargado:\n",
        "NER_TAGS.vocab.itos"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<pad>',\n",
              " 'O',\n",
              " 'I-Disease',\n",
              " 'B-Disease',\n",
              " 'I-Body_Part',\n",
              " 'B-Body_Part',\n",
              " 'B-Procedure',\n",
              " 'I-Procedure',\n",
              " 'B-Medication',\n",
              " 'B-Family_Member',\n",
              " 'I-Medication',\n",
              " 'I-Family_Member']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYQDoUqSHFKj"
      },
      "source": [
        "Observen que ademas de los tags NER, tenemos \\<pad\\>, el cual es generado por el dataloader para cumplir con el padding de cada oración.\n",
        "\n",
        "Veamos ahora los tokens mas frecuentes y especiales:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5eSLm4diibR",
        "outputId": "7d93610f-8b7e-472f-d8c4-d7f79e04b945"
      },
      "source": [
        "# Tokens mas frecuentes (Será necesario usar stopwords, eliminar símbolos o nos entregan información (?) )\n",
        "TEXT.vocab.freqs.most_common(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('.', 7396),\n",
              " (',', 6821),\n",
              " ('-', 4985),\n",
              " ('de', 3811),\n",
              " ('DE', 3645),\n",
              " ('/', 2317),\n",
              " (':', 2209),\n",
              " ('con', 1484),\n",
              " ('y', 1439),\n",
              " ('APS', 1429)]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDyNLMPz9duD"
      },
      "source": [
        "# Seteamos algunas variables que nos serán de utilidad mas adelante...\n",
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "PAD_TAG_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "O_TAG_IDX = NER_TAGS.vocab.stoi['O']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrYvF3X0sjWL"
      },
      "source": [
        "#### **Frecuencia de los Tags**\n",
        "\n",
        "Visualizemos rápidamente las cantidades y frecuencias de cada tag:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuXOsbJUiibh",
        "outputId": "af746331-2217-4af7-84e6-8395faa1484b"
      },
      "source": [
        "def tag_percentage(tag_counts):\n",
        "    \n",
        "    total_count = sum([count for tag, count in tag_counts])\n",
        "    tag_counts_percentages = [(tag, count, count/total_count) for tag, count in tag_counts]\n",
        "  \n",
        "    return tag_counts_percentages\n",
        "\n",
        "print(\"Tag Ocurrencia Porcentaje\\n\")\n",
        "\n",
        "for tag, count, percent in tag_percentage(NER_TAGS.vocab.freqs.most_common()):\n",
        "    print(f\"{tag}\\t{count}\\t{percent*100:4.1f}%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag Ocurrencia Porcentaje\n",
            "\n",
            "O\t101671\t68.1%\n",
            "I-Disease\t21629\t14.5%\n",
            "B-Disease\t8831\t 5.9%\n",
            "I-Body_Part\t6489\t 4.3%\n",
            "B-Body_Part\t3755\t 2.5%\n",
            "B-Procedure\t2891\t 1.9%\n",
            "I-Procedure\t2819\t 1.9%\n",
            "B-Medication\t784\t 0.5%\n",
            "B-Family_Member\t228\t 0.2%\n",
            "I-Medication\t116\t 0.1%\n",
            "I-Family_Member\t9\t 0.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4wPiydnaSGs"
      },
      "source": [
        "#### **Configuramos pytorch y dividimos los datos.**\n",
        "\n",
        "Importante: si tienes problemas con la ram de la gpu, disminuye el tamaño de los batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uB7cwLWpaSGs",
        "outputId": "c6285dc0-8c51-4f6e-950b-77b8e70d35f8"
      },
      "source": [
        "BATCH_SIZE = 16  # disminuir si hay problemas de ram.\n",
        "\n",
        "# Usar cuda si es que está disponible.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using', device)\n",
        "\n",
        "# Dividir datos entre entrenamiento y test. Si van a hacer algún sort no puede ser sobre\n",
        "# el conjunto de testing ya que al hacer sus predicciones sobre el conjunto de test sin etiquetas\n",
        "# debe conservar el orden original para ser comparado con los golden_labels. \n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = legacy.data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    device=device,\n",
        "    sort=False,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B21E1eAFId16"
      },
      "source": [
        "#### **Métricas de evaluación**\n",
        "\n",
        "Además, definiremos las métricas que serán usadas tanto para la competencia como para evaluar el modelo: `precision`, `recall` y `micro f1-score`.\n",
        "**Importante**: Noten que la evaluación solo se hace para las Named Entities (sin contar 'O'), toda esta funcionalidad nos la entrega la librería seqeval, pueden revisar más documentación aquí: https://github.com/chakki-works/seqeval. No utilicen el código entregado por sklearn para calcular las métricas ya que esta lo hace a nivel de token y no a nivel de entidad."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o63ov69_rX2T",
        "outputId": "a85db976-be0b-4a05-fd5c-268d551c74b6"
      },
      "source": [
        "!pip install seqeval"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.7/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mUOOLEWiicU"
      },
      "source": [
        "# Definimos las métricas\n",
        "\n",
        "from seqeval.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "def calculate_metrics(preds, y_true, pad_idx=PAD_TAG_IDX, o_idx=O_TAG_IDX):\n",
        "    \"\"\"\n",
        "    Calcula precision, recall y f1 de cada batch.\n",
        "    \"\"\"\n",
        "\n",
        "    # Obtener el indice de la clase con probabilidad mayor. (clases)\n",
        "    y_pred = preds.argmax(dim=1, keepdim=True)\n",
        "\n",
        "    # filtramos <pad> para calcular los scores.\n",
        "    mask = [(y_true != pad_idx)]\n",
        "    y_pred = y_pred[mask]\n",
        "    y_true = y_true[mask]\n",
        "\n",
        "    # traemos a la cpu\n",
        "    y_pred = y_pred.view(-1).to('cpu').numpy()\n",
        "    y_true = y_true.to('cpu').numpy()\n",
        "    y_pred = [[NER_TAGS.vocab.itos[v] for v in y_pred]]\n",
        "    y_true = [[NER_TAGS.vocab.itos[v] for v in y_true]]\n",
        "    \n",
        "    # calcular scores\n",
        "    f1 = f1_score(y_true, y_pred, mode='strict')\n",
        "    precision = precision_score(y_true, y_pred, mode='strict')\n",
        "    recall = recall_score(y_true, y_pred, mode='strict')\n",
        "\n",
        "    return precision, recall, f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hod516H1aSG2"
      },
      "source": [
        "-------------------\n",
        "\n",
        "### **Modelo Baseline**\n",
        "\n",
        "Teniendo ya cargado los datos, toca definir nuestro modelo. Este baseline tendrá una capa de embedding, unas cuantas LSTM y una capa de salida y usará dropout en el entrenamiento.\n",
        "\n",
        "Este constará de los siguientes pasos: \n",
        "\n",
        "1. Definir la clase que contendrá la red.\n",
        "2. Definir los hiperparámetros e inicializar la red. \n",
        "3. Definir el número de épocas de entrenamiento\n",
        "4. Definir la función de loss.\n",
        "\n",
        "\n",
        "\n",
        "Recomendamos que para experimentar, encapsules los modelos en una sola variable y luego la fijes en model para entrenarla"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMPL08XqaSG3"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# Definir la red\n",
        "class NER_RNN(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 embedding_dim, \n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 n_layers, \n",
        "                 bidirectional, \n",
        "                 dropout, \n",
        "                 pad_idx):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Capa de embedding\n",
        "        self.embedding = nn.Embedding(input_dim,\n",
        "                                      embedding_dim,\n",
        "                                      padding_idx=pad_idx)\n",
        "\n",
        "        # Capa LSTM\n",
        "        self.lstm = nn.LSTM(embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout = dropout if n_layers > 1 else 0)\n",
        "\n",
        "        # Capa de salida\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "\n",
        "        # Convertir lo enviado a embedding\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        \n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "\n",
        "        # Pasar los embeddings por la rnn (LSTM)\n",
        "\n",
        "        #output = [sent len, batch size, hid dim * n directions]\n",
        "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        # Predecir usando la capa de salida.\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "\n",
        "        return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCl3530VaSG7"
      },
      "source": [
        "#### **Hiperparámetros de la red**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHdi3QdOaSG8"
      },
      "source": [
        "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 300  # dimensión de los embeddings. 200\n",
        "HIDDEN_DIM = 256  # dimensión de la capas LSTM 128\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
        "\n",
        "N_LAYERS = 3  # número de capas.\n",
        "DROPOUT = 0.3  # 0.3\n",
        "BIDIRECTIONAL = False\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "baseline_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "baseline_model_name = 'baseline'  # nombre que tendrá el modelo guardado...."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlF1DhJeaSHA"
      },
      "source": [
        "baseline_n_epochs = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3u4imJGaSHE"
      },
      "source": [
        "#### Definimos la función de loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G_4k99_aSHG"
      },
      "source": [
        "# Loss: Cross Entropy\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "baseline_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPGdirx7aSHZ"
      },
      "source": [
        "------\n",
        "## **Entrenamos y evaluamos**\n",
        "\n",
        "\n",
        "**Importante** : Fijen el modelo, el número de épocas de entrenamiento, la loss y el optimizador que usarán para entrenar y evaluar en las siguientes variables!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8YlGnjxaSHZ"
      },
      "source": [
        "model = baseline_model\n",
        "model_name = baseline_model_name\n",
        "criterion = baseline_criterion\n",
        "n_epochs = baseline_n_epochs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu_lXic2aSHd"
      },
      "source": [
        "\n",
        "\n",
        "#### **Inicializamos la red**\n",
        "\n",
        "Iniciamos los pesos de la red de forma aleatoria (Usando una distribución normal).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-G_NWFcaSHe",
        "outputId": "b27af66a-fefd-49cf-d730-53a9342f9d15"
      },
      "source": [
        "def init_weights(m):\n",
        "    # Inicializamos los pesos como aleatorios\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean=0, std=0.1) \n",
        "        \n",
        "    # Seteamos como 0 los embeddings de UNK y PAD\n",
        "    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NER_RNN(\n",
              "  (embedding): Embedding(17591, 300, padding_idx=1)\n",
              "  (lstm): LSTM(300, 256, num_layers=3, dropout=0.3)\n",
              "  (fc): Linear(in_features=256, out_features=12, bias=True)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjWDX2CJaSHh",
        "outputId": "2a08d25e-c680-4134-fea7-6bd93fd7581c"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El modelo actual tiene 6,904,448 parámetros entrenables.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVqBqerlaSHk"
      },
      "source": [
        "Notar que definimos los embeddings que representan a \\<unk\\> y \\<pad\\>  como [0, 0, ..., 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVZvHtwpaSHq"
      },
      "source": [
        "#### **Definimos el optimizador**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AH6o8_cTaSHq"
      },
      "source": [
        "# Optimizador\n",
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz39wa78wGYR"
      },
      "source": [
        "#### **Enviamos el modelo a cuda**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqr0AJ6_iicR"
      },
      "source": [
        "# Enviamos el modelo y la loss a cuda (en el caso en que esté disponible)\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xlq48WjiW6U"
      },
      "source": [
        "#### **Definimos el entrenamiento de la red**\n",
        "\n",
        "Algunos conceptos previos: \n",
        "\n",
        "- `epoch` : una pasada de entrenamiento completa de una dataset.\n",
        "- `batch`: una fracción de la época. Se utilizan para entrenar mas rápidamente la red. (mas eficiente pasar n datos que uno en cada ejecución del backpropagation)\n",
        "\n",
        "Esta función está encargada de entrenar la red en una época. Para esto, por cada batch de la época actual, predice los tags del texto, calcula su loss y luego hace backpropagation para actualizar los pesos de la red.\n",
        "\n",
        "Observación: En algunos comentarios aparecerá el tamaño de los tensores entre corchetes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DV6YLt0oiicW"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_precision = 0\n",
        "    epoch_recall = 0\n",
        "    epoch_f1 = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Por cada batch del iterador de la época:\n",
        "    for batch in iterator:\n",
        "\n",
        "        # Extraemos el texto y los tags del batch que estamos procesado\n",
        "        text = batch.text\n",
        "        tags = batch.nertags\n",
        "\n",
        "        # Reiniciamos los gradientes calculados en la iteración anterior\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "\n",
        "        # Predecimos los tags del texto del batch.\n",
        "        predictions = model(text)\n",
        "\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "        #tags = [sent len, batch size]\n",
        "\n",
        "        # Reordenamos los datos para calcular la loss\n",
        "        predictions = predictions.view(-1, predictions.shape[-1])\n",
        "        tags = tags.view(-1)\n",
        "\n",
        "        #predictions = [sent len * batch size, output dim]\n",
        "\n",
        "\n",
        "\n",
        "        # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n",
        "        loss = criterion(predictions, tags)\n",
        "        \n",
        "        # Calculamos el accuracy\n",
        "        precision, recall, f1 = calculate_metrics(predictions, tags)\n",
        "\n",
        "        # Calculamos los gradientes\n",
        "        loss.backward()\n",
        "\n",
        "        # Actualizamos los parámetros de la red\n",
        "        optimizer.step()\n",
        "\n",
        "        # Actualizamos el loss y las métricas\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_precision += precision\n",
        "        epoch_recall += recall\n",
        "        epoch_f1 += f1\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_precision / len(\n",
        "        iterator), epoch_recall / len(iterator), epoch_f1 / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYNcwKnAz5Hf"
      },
      "source": [
        "#### **Definimos la función de evaluación**\n",
        "\n",
        "Evalua el rendimiento actual de la red usando los datos de validación. \n",
        "\n",
        "Por cada batch de estos datos, calcula y reporta el loss y las métricas asociadas al conjunto de validación. \n",
        "Ya que las métricas son calculadas por cada batch, estas son retornadas promediadas por el número de batches entregados. (ver linea del return)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsRuiUuHiicY"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_precision = 0\n",
        "    epoch_recall = 0\n",
        "    epoch_f1 = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Indicamos que ahora no guardaremos los gradientes\n",
        "    with torch.no_grad():\n",
        "        # Por cada batch\n",
        "        for batch in iterator:\n",
        "\n",
        "            text = batch.text\n",
        "            tags = batch.nertags\n",
        "\n",
        "            # Predecimos\n",
        "            predictions = model(text)\n",
        "\n",
        "            predictions = predictions.view(-1, predictions.shape[-1])\n",
        "            tags = tags.view(-1)\n",
        "\n",
        "            # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n",
        "            loss = criterion(predictions, tags)\n",
        "\n",
        "            # Calculamos las métricas\n",
        "            precision, recall, f1 = calculate_metrics(predictions, tags)\n",
        "\n",
        "            # Actualizamos el loss y las métricas\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_precision += precision\n",
        "            epoch_recall += recall\n",
        "            epoch_f1 += f1\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_precision / len(\n",
        "        iterator), epoch_recall / len(iterator), epoch_f1 / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xs-n9Y5yiica"
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hy3MVf5H0A94"
      },
      "source": [
        "\n",
        "#### **Entrenamiento de la red**\n",
        "\n",
        "En este cuadro de código ejecutaremos el entrenamiento de la red.\n",
        "Para esto, primero definiremos el número de épocas y luego por cada época, ejecutaremos `train` y `evaluate`.\n",
        "\n",
        "**Importante: Reiniciar los pesos del modelo**\n",
        "\n",
        "Si ejecutas nuevamente esta celda, se seguira entrenando el mismo modelo una y otra vez. \n",
        "Para reiniciar el modelo se debe ejecutar nuevamente la celda que contiene la función `init_weights`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iK5lQqpviicf",
        "outputId": "e9f6b364-5858-4ed3-a987-79c40c684e5d"
      },
      "source": [
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Recuerdo: train_iterator y valid_iterator contienen el dataset dividido en batches.\n",
        "\n",
        "    # Entrenar\n",
        "    train_loss, train_precision, train_recall, train_f1 = train(\n",
        "        model, train_iterator, optimizer, criterion)\n",
        "\n",
        "    # Evaluar (valid = validación)\n",
        "    valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
        "        model, valid_iterator, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    # Si obtuvimos mejores resultados, guardamos este modelo en el almacenamiento (para poder cargarlo luego)\n",
        "    # Si detienen el entrenamiento prematuramente, pueden cargar el modelo en el siguiente recuadro de código.\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), '{}.pt'.format(model_name))\n",
        "    # Si ya no mejoramos el loss de validación, terminamos de entrenar.\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(\n",
        "        f'\\tTrain Loss: {train_loss:.3f} | Train f1: {train_f1:.2f} | Train precision: {train_precision:.2f} | Train recall: {train_recall:.2f}'\n",
        "    )\n",
        "    print(\n",
        "        f'\\t Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} |  Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.748 | Train f1: 0.44 | Train precision: 0.59 | Train recall: 0.37\n",
            "\t Val. Loss: 0.469 |  Val. f1: 0.65 |  Val. precision: 0.74 | Val. recall: 0.58\n",
            "Epoch: 02 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.368 | Train f1: 0.74 | Train precision: 0.77 | Train recall: 0.71\n",
            "\t Val. Loss: 0.406 |  Val. f1: 0.70 |  Val. precision: 0.71 | Val. recall: 0.70\n",
            "Epoch: 03 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.246 | Train f1: 0.82 | Train precision: 0.83 | Train recall: 0.81\n",
            "\t Val. Loss: 0.381 |  Val. f1: 0.74 |  Val. precision: 0.76 | Val. recall: 0.73\n",
            "Epoch: 04 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.185 | Train f1: 0.86 | Train precision: 0.87 | Train recall: 0.86\n",
            "\t Val. Loss: 0.388 |  Val. f1: 0.74 |  Val. precision: 0.77 | Val. recall: 0.71\n",
            "Epoch: 05 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.149 | Train f1: 0.89 | Train precision: 0.89 | Train recall: 0.89\n",
            "\t Val. Loss: 0.431 |  Val. f1: 0.72 |  Val. precision: 0.70 | Val. recall: 0.75\n",
            "Epoch: 06 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.122 | Train f1: 0.91 | Train precision: 0.91 | Train recall: 0.91\n",
            "\t Val. Loss: 0.432 |  Val. f1: 0.74 |  Val. precision: 0.74 | Val. recall: 0.75\n",
            "Epoch: 07 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.103 | Train f1: 0.92 | Train precision: 0.92 | Train recall: 0.92\n",
            "\t Val. Loss: 0.478 |  Val. f1: 0.74 |  Val. precision: 0.75 | Val. recall: 0.74\n",
            "Epoch: 08 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.090 | Train f1: 0.93 | Train precision: 0.93 | Train recall: 0.94\n",
            "\t Val. Loss: 0.504 |  Val. f1: 0.73 |  Val. precision: 0.73 | Val. recall: 0.73\n",
            "Epoch: 09 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.077 | Train f1: 0.94 | Train precision: 0.94 | Train recall: 0.94\n",
            "\t Val. Loss: 0.526 |  Val. f1: 0.74 |  Val. precision: 0.75 | Val. recall: 0.73\n",
            "Epoch: 10 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.070 | Train f1: 0.94 | Train precision: 0.94 | Train recall: 0.95\n",
            "\t Val. Loss: 0.563 |  Val. f1: 0.73 |  Val. precision: 0.73 | Val. recall: 0.74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcZPraG-9duO"
      },
      "source": [
        "**Importante**: Recuerden que el último modelo entrenado no es el mejor (probablemente esté *overfitteado*), si no el que guardamos con la menor loss del conjunto de validación. Este problema lo pueden solucionar con *early stopping*.\n",
        "Para cargar el mejor modelo entrenado, ejecuten la siguiente celda.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y27CNYfrjtQ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c81258be-dc47-4c93-8c62-3e9fcf2b8959"
      },
      "source": [
        "# cargar el mejor modelo entrenado.\n",
        "model.load_state_dict(torch.load('{}.pt'.format(model_name)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLuqFKFR9duO"
      },
      "source": [
        "# Limpiar ram de cuda\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBctQHTh0lxD"
      },
      "source": [
        "#### **Evaluamos el set de validación con el modelo final**\n",
        "\n",
        "Estos son los resultados de predecir el dataset de evaluación con el *mejor* modelo entrenado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0gVbP8yiicj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "864b6250-bad2-40eb-9cdc-3ba75a0d0131"
      },
      "source": [
        "valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
        "    model, valid_iterator, criterion)\n",
        "\n",
        "print(\n",
        "    f'Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} | Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val. Loss: 0.381 |  Val. f1: 0.74 | Val. precision: 0.76 | Val. recall: 0.73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Experimentacion EarlyStopping\n",
        "La primera experimentacion que se propone es la utilizacion de earlyStopping para normalizar y evitar el overfitting ademas de proveer la facilidad de no tener que utilizar tiempo en exceso al experimentar con las cantidad de epocas."
      ],
      "metadata": {
        "id": "xdcAY2a0Z8Kh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Los argumetnos de la funciones son el modelo fijado, y el argumento stop indica cuando debe terminar el entrenamiento cuando no da mejor loss\n",
        "def earlyStop_model(model, train_iterator, valid_iterator, optimizer, \n",
        "                   criterion, stop = 10):\n",
        "\n",
        "  #nro de epocas\n",
        "  Epochs = 100\n",
        "  counter = 0\n",
        "  best_epoch = 0\n",
        "  best_valid_loss = float('inf')\n",
        "  last_valid_loss = float('inf')\n",
        "  \n",
        "  for epoch in range(Epochs):\n",
        "    \n",
        "      start_time = time.time()\n",
        "\n",
        "      # Recuerdo: train_iterator y valid_iterator contienen el dataset dividido en batches.\n",
        "\n",
        "      # Entrenar\n",
        "      train_loss, train_precision, train_recall, train_f1 = train(\n",
        "          model, train_iterator, optimizer, criterion)\n",
        "\n",
        "      # Evaluar (valid = validación)\n",
        "      valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
        "          model, valid_iterator, criterion)\n",
        "\n",
        "      end_time = time.time()\n",
        "\n",
        "      epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "      print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "      print(\n",
        "          f'\\tTrain Loss: {train_loss:.3f} | Train f1: {train_f1:.2f} | Train precision: {train_precision:.2f} | Train recall: {train_recall:.2f}'\n",
        "      )\n",
        "      print(\n",
        "          f'\\t Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} |  Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
        "      )\n",
        "\n",
        "      # Si obtuvimos mejores resultados, guardamos este modelo en el almacenamiento (para poder cargarlo luego)\n",
        "      # Si detienen el entrenamiento prematuramente, pueden cargar el modelo en el siguiente recuadro de código.\n",
        "      if valid_loss < best_valid_loss:\n",
        "          best_valid_loss = valid_loss\n",
        "          torch.save(model.state_dict(), '{}.pt'.format(model_name))\n",
        "          best_epoch = epoch\n",
        "          counter = 0\n",
        "\n",
        "      # Si ya no mejoramos el loss de validación, terminamos de entrenar.\n",
        "      else:\n",
        "          counter += 1\n",
        "                   \n",
        "          if counter == stop:\n",
        "              break\n",
        "\n",
        "      last_valid_loss = valid_loss\n",
        "\n",
        "  # cargar el mejor modelo entrenado.\n",
        "  model.load_state_dict(torch.load('{}.pt'.format(model_name)))\n",
        "  # Limpiar ram de cuda\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
        "    model, valid_iterator, criterion)\n",
        "  \n",
        "  print(f'\\nBest Model:')\n",
        "  print(\n",
        "      f'Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} | Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
        "  )"
      ],
      "metadata": {
        "id": "zz7f3BGsbbDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Ejemplo de uso con el modelo Baseline"
      ],
      "metadata": {
        "id": "kJBO0ddDjD56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.apply(init_weights)\n",
        "earlyStop_model(model, train_iterator, valid_iterator, optimizer, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kf92PLZWe1gI",
        "outputId": "1642dc05-2056-4d86-8d6d-fcd9f5eb1f57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 1.028 | Train f1: 0.17 | Train precision: 0.44 | Train recall: 0.11\n",
            "\t Val. Loss: 0.718 |  Val. f1: 0.45 |  Val. precision: 0.70 | Val. recall: 0.34\n",
            "Epoch: 02 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: 0.512 | Train f1: 0.64 | Train precision: 0.73 | Train recall: 0.57\n",
            "\t Val. Loss: 0.440 |  Val. f1: 0.67 |  Val. precision: 0.77 | Val. recall: 0.61\n",
            "Epoch: 03 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.298 | Train f1: 0.78 | Train precision: 0.80 | Train recall: 0.77\n",
            "\t Val. Loss: 0.381 |  Val. f1: 0.73 |  Val. precision: 0.73 | Val. recall: 0.72\n",
            "Epoch: 04 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.212 | Train f1: 0.84 | Train precision: 0.85 | Train recall: 0.84\n",
            "\t Val. Loss: 0.383 |  Val. f1: 0.74 |  Val. precision: 0.75 | Val. recall: 0.74\n",
            "Epoch: 05 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.164 | Train f1: 0.88 | Train precision: 0.88 | Train recall: 0.88\n",
            "\t Val. Loss: 0.412 |  Val. f1: 0.74 |  Val. precision: 0.75 | Val. recall: 0.73\n",
            "Epoch: 06 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.136 | Train f1: 0.90 | Train precision: 0.90 | Train recall: 0.90\n",
            "\t Val. Loss: 0.423 |  Val. f1: 0.73 |  Val. precision: 0.73 | Val. recall: 0.73\n",
            "Epoch: 07 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.115 | Train f1: 0.91 | Train precision: 0.91 | Train recall: 0.92\n",
            "\t Val. Loss: 0.469 |  Val. f1: 0.73 |  Val. precision: 0.74 | Val. recall: 0.73\n",
            "Epoch: 08 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.098 | Train f1: 0.93 | Train precision: 0.92 | Train recall: 0.93\n",
            "\t Val. Loss: 0.475 |  Val. f1: 0.73 |  Val. precision: 0.75 | Val. recall: 0.72\n",
            "Epoch: 09 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.086 | Train f1: 0.93 | Train precision: 0.93 | Train recall: 0.94\n",
            "\t Val. Loss: 0.498 |  Val. f1: 0.73 |  Val. precision: 0.74 | Val. recall: 0.73\n",
            "Epoch: 10 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.080 | Train f1: 0.94 | Train precision: 0.94 | Train recall: 0.94\n",
            "\t Val. Loss: 0.511 |  Val. f1: 0.72 |  Val. precision: 0.72 | Val. recall: 0.73\n",
            "Epoch: 11 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.069 | Train f1: 0.95 | Train precision: 0.94 | Train recall: 0.95\n",
            "\t Val. Loss: 0.534 |  Val. f1: 0.73 |  Val. precision: 0.74 | Val. recall: 0.73\n",
            "Epoch: 12 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.060 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.96\n",
            "\t Val. Loss: 0.543 |  Val. f1: 0.73 |  Val. precision: 0.73 | Val. recall: 0.73\n",
            "Epoch: 13 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.055 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.96\n",
            "\t Val. Loss: 0.608 |  Val. f1: 0.73 |  Val. precision: 0.73 | Val. recall: 0.74\n",
            "\n",
            "Best Model:\n",
            "Val. Loss: 0.381 |  Val. f1: 0.73 | Val. precision: 0.73 | Val. recall: 0.72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Experimentacion modelos Elman y GRU\n",
        "Como alternativa a las redes recurentes LSTM, se procede a experimentar con modelos GRU y Elmann unidireccionales ya que estas ya se encuentran implementadas en Pytorch, ademas de utilizar la funcion earlyStop_model para evitar el overfitting."
      ],
      "metadata": {
        "id": "db9v8JonkU_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ELMAN(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 embedding_dim, \n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 n_layers, \n",
        "                 bidirectional, \n",
        "                 dropout, \n",
        "                 pad_idx,\n",
        "                 nonlinearity = 'relu'):     #Por defecto se usa tanh pero para experimenter la cambie a relu\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Capa de embedding\n",
        "        self.embedding = nn.Embedding(input_dim,\n",
        "                                      embedding_dim,\n",
        "                                      padding_idx=pad_idx)\n",
        "\n",
        "        # Capa Elman RNN\n",
        "        self.rnn = nn.RNN(embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout = dropout if n_layers > 1 else 0,\n",
        "                           nonlinearity = nonlinearity)\n",
        "\n",
        "        # Capa de salida\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "\n",
        "        # Convertir lo enviado a embedding\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        \n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "\n",
        "        # Pasar los embeddings por la rnn (LSTM)\n",
        "\n",
        "        #output = [sent len, batch size, hid dim * n directions]\n",
        "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        # Predecir usando la capa de salida.\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "\n",
        "        return predictions\n",
        "\n",
        "class GRU(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 embedding_dim, \n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 n_layers, \n",
        "                 bidirectional, \n",
        "                 dropout, \n",
        "                 pad_idx):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Capa de embedding\n",
        "        self.embedding = nn.Embedding(input_dim,\n",
        "                                      embedding_dim,\n",
        "                                      padding_idx=pad_idx)\n",
        "\n",
        "        # Capa GRU RNN\n",
        "        self.gru = nn.GRU(embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout = dropout if n_layers > 1 else 0)\n",
        "\n",
        "        # Capa de salida\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "\n",
        "        # Convertir lo enviado a embedding\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        \n",
        "        outputs, hidden = self.gru(embedded)\n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "\n",
        "        # Pasar los embeddings por la rnn (LSTM)\n",
        "\n",
        "        #output = [sent len, batch size, hid dim * n directions]\n",
        "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        # Predecir usando la capa de salida.\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "\n",
        "        return predictions\n",
        "\n"
      ],
      "metadata": {
        "id": "12JS1hKljAIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Hiperparametros Elman y GRU"
      ],
      "metadata": {
        "id": "aZOVKZ93nzmI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SsAOS2vn1HA"
      },
      "source": [
        "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 300  # dimensión de los embeddings. \n",
        "HIDDEN_DIM = 256  # dimensión de la capas LSTM \n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
        "\n",
        "N_LAYERS = 3  # número de capas.\n",
        "DROPOUT = 0.3 \n",
        "BIDIRECTIONAL = False\n",
        "\n",
        "\n",
        "# Creamos nuestros modelos.\n",
        "elman_model = ELMAN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "elman_model_name = 'Elman'  # nombre que tendrá el modelo guardado...\n",
        "\n",
        "gru_model = GRU(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "gru_model_name = 'GRU'  # nombre que tendrá el modelo guardado..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Modelo Elman"
      ],
      "metadata": {
        "id": "4HOYzWzkwZTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = elman_model\n",
        "model_name = baseline_model_name\n",
        "criterion = baseline_criterion\n",
        "\n",
        "model.apply(init_weights)\n",
        "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "earlyStop_model(model, train_iterator, valid_iterator, optimizer, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPxMN0wWqgIw",
        "outputId": "eba3f68d-4b28-479f-97b7-52876973af9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El modelo actual tiene 5,686,400 parámetros entrenables.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 7s\n",
            "\tTrain Loss: 2.718 | Train f1: 0.05 | Train precision: 0.12 | Train recall: 0.04\n",
            "\t Val. Loss: 1.181 |  Val. f1: 0.00 |  Val. precision: 0.04 | Val. recall: 0.00\n",
            "Epoch: 02 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 1.130 | Train f1: 0.05 | Train precision: 0.33 | Train recall: 0.03\n",
            "\t Val. Loss: 1.125 |  Val. f1: 0.11 |  Val. precision: 0.50 | Val. recall: 0.06\n",
            "Epoch: 03 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 1.060 | Train f1: 0.11 | Train precision: 0.45 | Train recall: 0.07\n",
            "\t Val. Loss: 1.017 |  Val. f1: 0.21 |  Val. precision: 0.54 | Val. recall: 0.13\n",
            "Epoch: 04 | Epoch Time: 0m 7s\n",
            "\tTrain Loss: 0.954 | Train f1: 0.23 | Train precision: 0.53 | Train recall: 0.16\n",
            "\t Val. Loss: 0.886 |  Val. f1: 0.35 |  Val. precision: 0.60 | Val. recall: 0.25\n",
            "Epoch: 05 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.806 | Train f1: 0.38 | Train precision: 0.60 | Train recall: 0.28\n",
            "\t Val. Loss: 0.716 |  Val. f1: 0.45 |  Val. precision: 0.66 | Val. recall: 0.35\n",
            "Epoch: 06 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.662 | Train f1: 0.51 | Train precision: 0.67 | Train recall: 0.42\n",
            "\t Val. Loss: 0.617 |  Val. f1: 0.54 |  Val. precision: 0.66 | Val. recall: 0.47\n",
            "Epoch: 07 | Epoch Time: 0m 7s\n",
            "\tTrain Loss: 0.545 | Train f1: 0.60 | Train precision: 0.72 | Train recall: 0.52\n",
            "\t Val. Loss: 0.535 |  Val. f1: 0.62 |  Val. precision: 0.72 | Val. recall: 0.55\n",
            "Epoch: 08 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.452 | Train f1: 0.67 | Train precision: 0.75 | Train recall: 0.61\n",
            "\t Val. Loss: 0.507 |  Val. f1: 0.64 |  Val. precision: 0.70 | Val. recall: 0.60\n",
            "Epoch: 09 | Epoch Time: 0m 7s\n",
            "\tTrain Loss: 0.384 | Train f1: 0.71 | Train precision: 0.76 | Train recall: 0.66\n",
            "\t Val. Loss: 0.483 |  Val. f1: 0.66 |  Val. precision: 0.69 | Val. recall: 0.64\n",
            "Epoch: 10 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.338 | Train f1: 0.74 | Train precision: 0.78 | Train recall: 0.71\n",
            "\t Val. Loss: 0.468 |  Val. f1: 0.69 |  Val. precision: 0.71 | Val. recall: 0.66\n",
            "Epoch: 11 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.295 | Train f1: 0.77 | Train precision: 0.80 | Train recall: 0.75\n",
            "\t Val. Loss: 0.474 |  Val. f1: 0.69 |  Val. precision: 0.69 | Val. recall: 0.69\n",
            "Epoch: 12 | Epoch Time: 0m 7s\n",
            "\tTrain Loss: 0.260 | Train f1: 0.80 | Train precision: 0.82 | Train recall: 0.79\n",
            "\t Val. Loss: 0.468 |  Val. f1: 0.70 |  Val. precision: 0.71 | Val. recall: 0.69\n",
            "Epoch: 13 | Epoch Time: 0m 7s\n",
            "\tTrain Loss: 0.234 | Train f1: 0.83 | Train precision: 0.84 | Train recall: 0.82\n",
            "\t Val. Loss: 0.485 |  Val. f1: 0.70 |  Val. precision: 0.72 | Val. recall: 0.70\n",
            "Epoch: 14 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.207 | Train f1: 0.85 | Train precision: 0.85 | Train recall: 0.84\n",
            "\t Val. Loss: 0.478 |  Val. f1: 0.71 |  Val. precision: 0.72 | Val. recall: 0.70\n",
            "Epoch: 15 | Epoch Time: 0m 7s\n",
            "\tTrain Loss: 0.183 | Train f1: 0.86 | Train precision: 0.87 | Train recall: 0.86\n",
            "\t Val. Loss: 0.469 |  Val. f1: 0.73 |  Val. precision: 0.76 | Val. recall: 0.71\n",
            "Epoch: 16 | Epoch Time: 0m 7s\n",
            "\tTrain Loss: 0.165 | Train f1: 0.87 | Train precision: 0.88 | Train recall: 0.87\n",
            "\t Val. Loss: 0.500 |  Val. f1: 0.72 |  Val. precision: 0.75 | Val. recall: 0.70\n",
            "Epoch: 17 | Epoch Time: 0m 7s\n",
            "\tTrain Loss: 0.152 | Train f1: 0.89 | Train precision: 0.89 | Train recall: 0.89\n",
            "\t Val. Loss: 0.502 |  Val. f1: 0.72 |  Val. precision: 0.75 | Val. recall: 0.70\n",
            "Epoch: 18 | Epoch Time: 0m 7s\n",
            "\tTrain Loss: 0.144 | Train f1: 0.89 | Train precision: 0.90 | Train recall: 0.89\n",
            "\t Val. Loss: 0.535 |  Val. f1: 0.72 |  Val. precision: 0.74 | Val. recall: 0.72\n",
            "Epoch: 19 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.130 | Train f1: 0.91 | Train precision: 0.91 | Train recall: 0.91\n",
            "\t Val. Loss: 0.537 |  Val. f1: 0.73 |  Val. precision: 0.73 | Val. recall: 0.73\n",
            "Epoch: 20 | Epoch Time: 0m 6s\n",
            "\tTrain Loss: 0.123 | Train f1: 0.91 | Train precision: 0.91 | Train recall: 0.91\n",
            "\t Val. Loss: 0.557 |  Val. f1: 0.72 |  Val. precision: 0.73 | Val. recall: 0.71\n",
            "\n",
            "Best Model:\n",
            "Val. Loss: 0.468 |  Val. f1: 0.69 | Val. precision: 0.71 | Val. recall: 0.66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Modelo GRU"
      ],
      "metadata": {
        "id": "KIEimAO_weRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = gru_model\n",
        "model_name = baseline_model_name\n",
        "criterion = baseline_criterion\n",
        "\n",
        "model.apply(init_weights)\n",
        "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "earlyStop_model(model, train_iterator, valid_iterator, optimizer, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoT2IzbKssqN",
        "outputId": "aec32e46-cb2a-4ef7-8948-b3fea1bbaf9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El modelo actual tiene 6,498,432 parámetros entrenables.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.756 | Train f1: 0.46 | Train precision: 0.59 | Train recall: 0.40\n",
            "\t Val. Loss: 0.450 |  Val. f1: 0.67 |  Val. precision: 0.75 | Val. recall: 0.61\n",
            "Epoch: 02 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.370 | Train f1: 0.73 | Train precision: 0.76 | Train recall: 0.70\n",
            "\t Val. Loss: 0.399 |  Val. f1: 0.72 |  Val. precision: 0.78 | Val. recall: 0.67\n",
            "Epoch: 03 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.244 | Train f1: 0.82 | Train precision: 0.83 | Train recall: 0.81\n",
            "\t Val. Loss: 0.409 |  Val. f1: 0.74 |  Val. precision: 0.75 | Val. recall: 0.74\n",
            "Epoch: 04 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.185 | Train f1: 0.86 | Train precision: 0.87 | Train recall: 0.86\n",
            "\t Val. Loss: 0.419 |  Val. f1: 0.74 |  Val. precision: 0.75 | Val. recall: 0.73\n",
            "Epoch: 05 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.148 | Train f1: 0.89 | Train precision: 0.89 | Train recall: 0.89\n",
            "\t Val. Loss: 0.452 |  Val. f1: 0.74 |  Val. precision: 0.75 | Val. recall: 0.73\n",
            "Epoch: 06 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.121 | Train f1: 0.91 | Train precision: 0.91 | Train recall: 0.91\n",
            "\t Val. Loss: 0.472 |  Val. f1: 0.75 |  Val. precision: 0.78 | Val. recall: 0.72\n",
            "Epoch: 07 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.106 | Train f1: 0.92 | Train precision: 0.92 | Train recall: 0.92\n",
            "\t Val. Loss: 0.490 |  Val. f1: 0.74 |  Val. precision: 0.77 | Val. recall: 0.72\n",
            "Epoch: 08 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.092 | Train f1: 0.93 | Train precision: 0.93 | Train recall: 0.93\n",
            "\t Val. Loss: 0.530 |  Val. f1: 0.73 |  Val. precision: 0.75 | Val. recall: 0.72\n",
            "Epoch: 09 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.084 | Train f1: 0.94 | Train precision: 0.93 | Train recall: 0.94\n",
            "\t Val. Loss: 0.522 |  Val. f1: 0.74 |  Val. precision: 0.75 | Val. recall: 0.73\n",
            "Epoch: 10 | Epoch Time: 0m 7s\n",
            "\tTrain Loss: 0.074 | Train f1: 0.94 | Train precision: 0.94 | Train recall: 0.94\n",
            "\t Val. Loss: 0.571 |  Val. f1: 0.73 |  Val. precision: 0.74 | Val. recall: 0.72\n",
            "Epoch: 11 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.068 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
            "\t Val. Loss: 0.594 |  Val. f1: 0.73 |  Val. precision: 0.75 | Val. recall: 0.72\n",
            "Epoch: 12 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.063 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
            "\t Val. Loss: 0.588 |  Val. f1: 0.73 |  Val. precision: 0.75 | Val. recall: 0.72\n",
            "\n",
            "Best Model:\n",
            "Val. Loss: 0.399 |  Val. f1: 0.72 | Val. precision: 0.78 | Val. recall: 0.67\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Experimentacion Bidireccionlidad\n",
        "Para esta seccion se estudiara el efecto de la bidereccionalidad de los modelos LSTM, GRU y Elmann."
      ],
      "metadata": {
        "id": "bHSJ0bVktE0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 300  # dimensión de los embeddings. \n",
        "HIDDEN_DIM = 256  # dimensión de la capas LSTM \n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
        "\n",
        "N_LAYERS = 3  # número de capas.\n",
        "DROPOUT = 0.3  \n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestros modelos.\n",
        "lstm_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "lstm_model_name = 'LSTMbid'  # nombre que tendrá el modelo guardado...\n",
        "\n",
        "elman_model = ELMAN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX, 'relu')\n",
        "\n",
        "elman_model_name = 'Elmanbid'  # nombre que tendrá el modelo guardado...\n",
        "\n",
        "gru_model = GRU(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "gru_model_name = 'GRUbid'  # nombre que tendrá el modelo guardado..."
      ],
      "metadata": {
        "id": "RrelLt2ft8N5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####LSTM-bidireccional"
      ],
      "metadata": {
        "id": "YbLvD4X6u1p4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = lstm_model\n",
        "model_name = lstm_model_name\n",
        "criterion = baseline_criterion\n",
        "\n",
        "#inicializamos los pesos\n",
        "model.apply(init_weights)\n",
        "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
        "\n",
        "#se determina el optimizador\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "earlyStop_model(model, train_iterator, valid_iterator, optimizer, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewjm3nQeu0d9",
        "outputId": "8c6ae16d-780e-4939-dce6-1e6b842900f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El modelo actual tiene 15,818,732 parámetros entrenables.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.634 | Train f1: 0.55 | Train precision: 0.66 | Train recall: 0.49\n",
            "\t Val. Loss: 0.397 |  Val. f1: 0.71 |  Val. precision: 0.78 | Val. recall: 0.65\n",
            "Epoch: 02 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.281 | Train f1: 0.80 | Train precision: 0.82 | Train recall: 0.78\n",
            "\t Val. Loss: 0.355 |  Val. f1: 0.76 |  Val. precision: 0.78 | Val. recall: 0.74\n",
            "Epoch: 03 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.158 | Train f1: 0.89 | Train precision: 0.89 | Train recall: 0.89\n",
            "\t Val. Loss: 0.374 |  Val. f1: 0.77 |  Val. precision: 0.78 | Val. recall: 0.76\n",
            "Epoch: 04 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.106 | Train f1: 0.92 | Train precision: 0.92 | Train recall: 0.92\n",
            "\t Val. Loss: 0.404 |  Val. f1: 0.76 |  Val. precision: 0.78 | Val. recall: 0.75\n",
            "Epoch: 05 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.078 | Train f1: 0.94 | Train precision: 0.94 | Train recall: 0.94\n",
            "\t Val. Loss: 0.419 |  Val. f1: 0.76 |  Val. precision: 0.75 | Val. recall: 0.77\n",
            "Epoch: 06 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.055 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
            "\t Val. Loss: 0.494 |  Val. f1: 0.76 |  Val. precision: 0.75 | Val. recall: 0.77\n",
            "Epoch: 07 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.043 | Train f1: 0.97 | Train precision: 0.97 | Train recall: 0.97\n",
            "\t Val. Loss: 0.559 |  Val. f1: 0.75 |  Val. precision: 0.74 | Val. recall: 0.76\n",
            "Epoch: 08 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.036 | Train f1: 0.97 | Train precision: 0.97 | Train recall: 0.97\n",
            "\t Val. Loss: 0.594 |  Val. f1: 0.75 |  Val. precision: 0.75 | Val. recall: 0.76\n",
            "Epoch: 09 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.030 | Train f1: 0.98 | Train precision: 0.98 | Train recall: 0.98\n",
            "\t Val. Loss: 0.559 |  Val. f1: 0.75 |  Val. precision: 0.75 | Val. recall: 0.76\n",
            "Epoch: 10 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.025 | Train f1: 0.98 | Train precision: 0.98 | Train recall: 0.98\n",
            "\t Val. Loss: 0.636 |  Val. f1: 0.74 |  Val. precision: 0.74 | Val. recall: 0.74\n",
            "Epoch: 11 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.020 | Train f1: 0.98 | Train precision: 0.98 | Train recall: 0.99\n",
            "\t Val. Loss: 0.650 |  Val. f1: 0.75 |  Val. precision: 0.76 | Val. recall: 0.74\n",
            "Epoch: 12 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.018 | Train f1: 0.99 | Train precision: 0.99 | Train recall: 0.99\n",
            "\t Val. Loss: 0.643 |  Val. f1: 0.75 |  Val. precision: 0.76 | Val. recall: 0.74\n",
            "\n",
            "Best Model:\n",
            "Val. Loss: 0.355 |  Val. f1: 0.76 | Val. precision: 0.78 | Val. recall: 0.74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Elman-bidireccional"
      ],
      "metadata": {
        "id": "-m9pt5FuvFoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = elman_model\n",
        "model_name = elman_model_name\n",
        "criterion = baseline_criterion\n",
        "#inicializar pesos\n",
        "model.apply(init_weights)\n",
        "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
        "#optimizador\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "earlyStop_model(model, train_iterator, valid_iterator, optimizer, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SllijQCHu8h-",
        "outputId": "94272b2a-fd98-4d3a-fdf1-76118a28c830"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El modelo actual tiene 6,357,632 parámetros entrenables.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: nan | Train f1: 0.04 | Train precision: 0.06 | Train recall: 0.04\n",
            "\t Val. Loss: nan |  Val. f1: 0.00 |  Val. precision: 0.00 | Val. recall: 0.00\n",
            "Epoch: 02 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: nan | Train f1: 0.00 | Train precision: 0.00 | Train recall: 0.00\n",
            "\t Val. Loss: nan |  Val. f1: 0.00 |  Val. precision: 0.00 | Val. recall: 0.00\n",
            "Epoch: 03 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: nan | Train f1: 0.00 | Train precision: 0.00 | Train recall: 0.00\n",
            "\t Val. Loss: nan |  Val. f1: 0.00 |  Val. precision: 0.00 | Val. recall: 0.00\n",
            "Epoch: 04 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: nan | Train f1: 0.00 | Train precision: 0.00 | Train recall: 0.00\n",
            "\t Val. Loss: nan |  Val. f1: 0.00 |  Val. precision: 0.00 | Val. recall: 0.00\n",
            "Epoch: 05 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: nan | Train f1: 0.00 | Train precision: 0.00 | Train recall: 0.00\n",
            "\t Val. Loss: nan |  Val. f1: 0.00 |  Val. precision: 0.00 | Val. recall: 0.00\n",
            "Epoch: 06 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: nan | Train f1: 0.00 | Train precision: 0.00 | Train recall: 0.00\n",
            "\t Val. Loss: nan |  Val. f1: 0.00 |  Val. precision: 0.00 | Val. recall: 0.00\n",
            "Epoch: 07 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: nan | Train f1: 0.00 | Train precision: 0.00 | Train recall: 0.00\n",
            "\t Val. Loss: nan |  Val. f1: 0.00 |  Val. precision: 0.00 | Val. recall: 0.00\n",
            "Epoch: 08 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: nan | Train f1: 0.00 | Train precision: 0.00 | Train recall: 0.00\n",
            "\t Val. Loss: nan |  Val. f1: 0.00 |  Val. precision: 0.00 | Val. recall: 0.00\n",
            "Epoch: 09 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: nan | Train f1: 0.00 | Train precision: 0.00 | Train recall: 0.00\n",
            "\t Val. Loss: nan |  Val. f1: 0.00 |  Val. precision: 0.00 | Val. recall: 0.00\n",
            "Epoch: 10 | Epoch Time: 0m 10s\n",
            "\tTrain Loss: nan | Train f1: 0.00 | Train precision: 0.00 | Train recall: 0.00\n",
            "\t Val. Loss: nan |  Val. f1: 0.00 |  Val. precision: 0.00 | Val. recall: 0.00\n",
            "\n",
            "Best Model:\n",
            "Val. Loss: 0.716 |  Val. f1: 0.52 | Val. precision: 0.62 | Val. recall: 0.45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####GRU-bidireccional"
      ],
      "metadata": {
        "id": "149yWdDpvJnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = gru_model\n",
        "model_name = gru_model_name\n",
        "criterion = baseline_criterion\n",
        "#inicializar pesos\n",
        "model.apply(init_weights)\n",
        "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
        "#optimizador\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "earlyStop_model(model, train_iterator, valid_iterator, optimizer, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74RqZ9kDvEaQ",
        "outputId": "8ea9e54c-1a6f-443b-91d8-800fbf0c7eb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El modelo actual tiene 8,505,984 parámetros entrenables.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.792 | Train f1: 0.48 | Train precision: 0.57 | Train recall: 0.43\n",
            "\t Val. Loss: 0.425 |  Val. f1: 0.69 |  Val. precision: 0.77 | Val. recall: 0.63\n",
            "Epoch: 02 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.357 | Train f1: 0.75 | Train precision: 0.78 | Train recall: 0.72\n",
            "\t Val. Loss: 0.366 |  Val. f1: 0.75 |  Val. precision: 0.78 | Val. recall: 0.73\n",
            "Epoch: 03 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.216 | Train f1: 0.84 | Train precision: 0.85 | Train recall: 0.83\n",
            "\t Val. Loss: 0.376 |  Val. f1: 0.75 |  Val. precision: 0.77 | Val. recall: 0.75\n",
            "Epoch: 04 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.148 | Train f1: 0.89 | Train precision: 0.89 | Train recall: 0.88\n",
            "\t Val. Loss: 0.404 |  Val. f1: 0.77 |  Val. precision: 0.80 | Val. recall: 0.74\n",
            "Epoch: 05 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.108 | Train f1: 0.92 | Train precision: 0.92 | Train recall: 0.92\n",
            "\t Val. Loss: 0.426 |  Val. f1: 0.77 |  Val. precision: 0.78 | Val. recall: 0.76\n",
            "Epoch: 06 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.086 | Train f1: 0.93 | Train precision: 0.94 | Train recall: 0.93\n",
            "\t Val. Loss: 0.442 |  Val. f1: 0.76 |  Val. precision: 0.76 | Val. recall: 0.76\n",
            "Epoch: 07 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.068 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
            "\t Val. Loss: 0.511 |  Val. f1: 0.76 |  Val. precision: 0.77 | Val. recall: 0.75\n",
            "Epoch: 08 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.057 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
            "\t Val. Loss: 0.517 |  Val. f1: 0.76 |  Val. precision: 0.78 | Val. recall: 0.75\n",
            "Epoch: 09 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.047 | Train f1: 0.96 | Train precision: 0.97 | Train recall: 0.96\n",
            "\t Val. Loss: 0.543 |  Val. f1: 0.76 |  Val. precision: 0.77 | Val. recall: 0.75\n",
            "Epoch: 10 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.043 | Train f1: 0.97 | Train precision: 0.97 | Train recall: 0.97\n",
            "\t Val. Loss: 0.560 |  Val. f1: 0.75 |  Val. precision: 0.76 | Val. recall: 0.74\n",
            "Epoch: 11 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.039 | Train f1: 0.97 | Train precision: 0.97 | Train recall: 0.97\n",
            "\t Val. Loss: 0.570 |  Val. f1: 0.76 |  Val. precision: 0.78 | Val. recall: 0.75\n",
            "Epoch: 12 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.034 | Train f1: 0.97 | Train precision: 0.97 | Train recall: 0.97\n",
            "\t Val. Loss: 0.610 |  Val. f1: 0.76 |  Val. precision: 0.77 | Val. recall: 0.75\n",
            "\n",
            "Best Model:\n",
            "Val. Loss: 0.366 |  Val. f1: 0.75 | Val. precision: 0.78 | Val. recall: 0.73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Experimentacion arquitecturas\n",
        "Para esta seccion se estudiara el efecto de distintas arquitecturas sobre las bidireccionales"
      ],
      "metadata": {
        "id": "ow1Pum6nTihF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####LSTM - bi - variando embedding"
      ],
      "metadata": {
        "id": "7JXxPKj1TihH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 500  # dimensión de los embeddings. \n",
        "HIDDEN_DIM = 256  # dimensión de la capas LSTM \n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
        "\n",
        "N_LAYERS = 3  # número de capas.\n",
        "DROPOUT = 0.3  \n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestros modelos.\n",
        "lstm_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "lstm_model_name = 'LSTMbidEMD'  # nombre que tendrá el modelo guardado..."
      ],
      "metadata": {
        "id": "IZ0N6C8YTihG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = lstm_model\n",
        "model_name = lstm_model_name\n",
        "criterion = baseline_criterion\n",
        "\n",
        "#inicializamos los pesos\n",
        "model.apply(init_weights)\n",
        "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
        "\n",
        "#se determina el optimizador\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "earlyStop_model(model, train_iterator, valid_iterator, optimizer, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63b6f4bb-80c7-4afe-dbdc-b7fe0aa5a426",
        "id": "DqaRSsL-TihH"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El modelo actual tiene 13,507,960 parámetros entrenables.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.646 | Train f1: 0.54 | Train precision: 0.66 | Train recall: 0.47\n",
            "\t Val. Loss: 0.390 |  Val. f1: 0.70 |  Val. precision: 0.78 | Val. recall: 0.64\n",
            "Epoch: 02 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.287 | Train f1: 0.79 | Train precision: 0.82 | Train recall: 0.77\n",
            "\t Val. Loss: 0.340 |  Val. f1: 0.76 |  Val. precision: 0.78 | Val. recall: 0.75\n",
            "Epoch: 03 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.170 | Train f1: 0.87 | Train precision: 0.88 | Train recall: 0.87\n",
            "\t Val. Loss: 0.353 |  Val. f1: 0.76 |  Val. precision: 0.76 | Val. recall: 0.77\n",
            "Epoch: 04 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.115 | Train f1: 0.91 | Train precision: 0.92 | Train recall: 0.91\n",
            "\t Val. Loss: 0.395 |  Val. f1: 0.77 |  Val. precision: 0.77 | Val. recall: 0.76\n",
            "Epoch: 05 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.079 | Train f1: 0.94 | Train precision: 0.94 | Train recall: 0.94\n",
            "\t Val. Loss: 0.439 |  Val. f1: 0.76 |  Val. precision: 0.77 | Val. recall: 0.75\n",
            "Epoch: 06 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.063 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
            "\t Val. Loss: 0.485 |  Val. f1: 0.76 |  Val. precision: 0.77 | Val. recall: 0.75\n",
            "Epoch: 07 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.047 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
            "\t Val. Loss: 0.531 |  Val. f1: 0.74 |  Val. precision: 0.73 | Val. recall: 0.75\n",
            "Epoch: 08 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.041 | Train f1: 0.97 | Train precision: 0.97 | Train recall: 0.97\n",
            "\t Val. Loss: 0.535 |  Val. f1: 0.75 |  Val. precision: 0.77 | Val. recall: 0.74\n",
            "Epoch: 09 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.036 | Train f1: 0.97 | Train precision: 0.97 | Train recall: 0.97\n",
            "\t Val. Loss: 0.566 |  Val. f1: 0.75 |  Val. precision: 0.76 | Val. recall: 0.74\n",
            "Epoch: 10 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.031 | Train f1: 0.98 | Train precision: 0.98 | Train recall: 0.98\n",
            "\t Val. Loss: 0.558 |  Val. f1: 0.75 |  Val. precision: 0.77 | Val. recall: 0.74\n",
            "Epoch: 11 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.025 | Train f1: 0.98 | Train precision: 0.98 | Train recall: 0.98\n",
            "\t Val. Loss: 0.642 |  Val. f1: 0.74 |  Val. precision: 0.75 | Val. recall: 0.74\n",
            "Epoch: 12 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.024 | Train f1: 0.98 | Train precision: 0.98 | Train recall: 0.98\n",
            "\t Val. Loss: 0.633 |  Val. f1: 0.76 |  Val. precision: 0.76 | Val. recall: 0.76\n",
            "\n",
            "Best Model:\n",
            "Val. Loss: 0.340 |  Val. f1: 0.76 | Val. precision: 0.78 | Val. recall: 0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####LSTM - bi - variando dimensión LSTMs = 512"
      ],
      "metadata": {
        "id": "Du5RLMZsUFcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 300  # dimensión de los embeddings. \n",
        "HIDDEN_DIM = 512  # dimensión de la capas LSTM \n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
        "\n",
        "N_LAYERS = 3  # número de capas.\n",
        "DROPOUT = 0.3  \n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestros modelos.\n",
        "lstm_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "lstm_model_name = 'LSTMbidHID'  # nombre que tendrá el modelo guardado..."
      ],
      "metadata": {
        "id": "NBUOcED5UFcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = lstm_model\n",
        "model_name = lstm_model_name\n",
        "criterion = baseline_criterion\n",
        "\n",
        "#inicializamos los pesos\n",
        "model.apply(init_weights)\n",
        "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
        "\n",
        "#se determina el optimizador\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "earlyStop_model(model, train_iterator, valid_iterator, optimizer, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "va0KxD_mUFch",
        "outputId": "02f37558-ab51-4f4e-e7df-530e17508958"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El modelo actual tiene 21,223,040 parámetros entrenables.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 28s\n",
            "\tTrain Loss: 0.843 | Train f1: 0.37 | Train precision: 0.56 | Train recall: 0.31\n",
            "\t Val. Loss: 0.500 |  Val. f1: 0.62 |  Val. precision: 0.74 | Val. recall: 0.54\n",
            "Epoch: 02 | Epoch Time: 0m 28s\n",
            "\tTrain Loss: 0.371 | Train f1: 0.74 | Train precision: 0.78 | Train recall: 0.70\n",
            "\t Val. Loss: 0.368 |  Val. f1: 0.74 |  Val. precision: 0.78 | Val. recall: 0.70\n",
            "Epoch: 03 | Epoch Time: 0m 29s\n",
            "\tTrain Loss: 0.222 | Train f1: 0.84 | Train precision: 0.85 | Train recall: 0.83\n",
            "\t Val. Loss: 0.368 |  Val. f1: 0.76 |  Val. precision: 0.79 | Val. recall: 0.74\n",
            "Epoch: 04 | Epoch Time: 0m 28s\n",
            "\tTrain Loss: 0.151 | Train f1: 0.89 | Train precision: 0.89 | Train recall: 0.89\n",
            "\t Val. Loss: 0.386 |  Val. f1: 0.75 |  Val. precision: 0.75 | Val. recall: 0.76\n",
            "Epoch: 05 | Epoch Time: 0m 29s\n",
            "\tTrain Loss: 0.112 | Train f1: 0.92 | Train precision: 0.92 | Train recall: 0.92\n",
            "\t Val. Loss: 0.405 |  Val. f1: 0.77 |  Val. precision: 0.79 | Val. recall: 0.75\n",
            "Epoch: 06 | Epoch Time: 0m 28s\n",
            "\tTrain Loss: 0.081 | Train f1: 0.94 | Train precision: 0.94 | Train recall: 0.94\n",
            "\t Val. Loss: 0.439 |  Val. f1: 0.77 |  Val. precision: 0.79 | Val. recall: 0.75\n",
            "Epoch: 07 | Epoch Time: 0m 29s\n",
            "\tTrain Loss: 0.066 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
            "\t Val. Loss: 0.465 |  Val. f1: 0.76 |  Val. precision: 0.78 | Val. recall: 0.76\n",
            "Epoch: 08 | Epoch Time: 0m 29s\n",
            "\tTrain Loss: 0.055 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
            "\t Val. Loss: 0.435 |  Val. f1: 0.76 |  Val. precision: 0.77 | Val. recall: 0.76\n",
            "Epoch: 09 | Epoch Time: 0m 29s\n",
            "\tTrain Loss: 0.048 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
            "\t Val. Loss: 0.517 |  Val. f1: 0.75 |  Val. precision: 0.76 | Val. recall: 0.75\n",
            "Epoch: 10 | Epoch Time: 0m 29s\n",
            "\tTrain Loss: 0.042 | Train f1: 0.97 | Train precision: 0.97 | Train recall: 0.97\n",
            "\t Val. Loss: 0.557 |  Val. f1: 0.76 |  Val. precision: 0.78 | Val. recall: 0.76\n",
            "Epoch: 11 | Epoch Time: 0m 28s\n",
            "\tTrain Loss: 0.037 | Train f1: 0.97 | Train precision: 0.97 | Train recall: 0.97\n",
            "\t Val. Loss: 0.562 |  Val. f1: 0.75 |  Val. precision: 0.76 | Val. recall: 0.75\n",
            "Epoch: 12 | Epoch Time: 0m 29s\n",
            "\tTrain Loss: 0.032 | Train f1: 0.97 | Train precision: 0.97 | Train recall: 0.98\n",
            "\t Val. Loss: 0.587 |  Val. f1: 0.75 |  Val. precision: 0.76 | Val. recall: 0.75\n",
            "Epoch: 13 | Epoch Time: 0m 28s\n",
            "\tTrain Loss: 0.027 | Train f1: 0.98 | Train precision: 0.98 | Train recall: 0.98\n",
            "\t Val. Loss: 0.599 |  Val. f1: 0.75 |  Val. precision: 0.76 | Val. recall: 0.75\n",
            "\n",
            "Best Model:\n",
            "Val. Loss: 0.368 |  Val. f1: 0.76 | Val. precision: 0.79 | Val. recall: 0.74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####LSTM - bi - variando dimensión LSTMs = 128"
      ],
      "metadata": {
        "id": "V-qO5XtcUZ1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 300  # dimensión de los embeddings. \n",
        "HIDDEN_DIM = 128  # dimensión de la capas LSTM \n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
        "\n",
        "N_LAYERS = 3  # número de capas.\n",
        "DROPOUT = 0.3  \n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestros modelos.\n",
        "lstm_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "lstm_model_name = 'LSTMbidDIM'  # nombre que tendrá el modelo guardado..."
      ],
      "metadata": {
        "id": "eZkjI5UOUZ1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = lstm_model\n",
        "model_name = lstm_model_name\n",
        "criterion = baseline_criterion\n",
        "\n",
        "#inicializamos los pesos\n",
        "model.apply(init_weights)\n",
        "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
        "\n",
        "#se determina el optimizador\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "earlyStop_model(model, train_iterator, valid_iterator, optimizer, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_Qlp_MpUZ1d",
        "outputId": "dd80095d-f015-43e3-c587-ddc17b4865f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El modelo actual tiene 6,511,232 parámetros entrenables.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.694 | Train f1: 0.50 | Train precision: 0.63 | Train recall: 0.43\n",
            "\t Val. Loss: 0.420 |  Val. f1: 0.69 |  Val. precision: 0.78 | Val. recall: 0.62\n",
            "Epoch: 02 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.322 | Train f1: 0.76 | Train precision: 0.80 | Train recall: 0.74\n",
            "\t Val. Loss: 0.357 |  Val. f1: 0.74 |  Val. precision: 0.76 | Val. recall: 0.73\n",
            "Epoch: 03 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.198 | Train f1: 0.86 | Train precision: 0.86 | Train recall: 0.85\n",
            "\t Val. Loss: 0.347 |  Val. f1: 0.76 |  Val. precision: 0.79 | Val. recall: 0.74\n",
            "Epoch: 04 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.137 | Train f1: 0.90 | Train precision: 0.90 | Train recall: 0.90\n",
            "\t Val. Loss: 0.374 |  Val. f1: 0.76 |  Val. precision: 0.79 | Val. recall: 0.75\n",
            "Epoch: 05 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.104 | Train f1: 0.92 | Train precision: 0.92 | Train recall: 0.92\n",
            "\t Val. Loss: 0.416 |  Val. f1: 0.77 |  Val. precision: 0.79 | Val. recall: 0.75\n",
            "Epoch: 06 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.079 | Train f1: 0.94 | Train precision: 0.94 | Train recall: 0.94\n",
            "\t Val. Loss: 0.442 |  Val. f1: 0.77 |  Val. precision: 0.77 | Val. recall: 0.76\n",
            "Epoch: 07 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.062 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
            "\t Val. Loss: 0.459 |  Val. f1: 0.76 |  Val. precision: 0.76 | Val. recall: 0.76\n",
            "Epoch: 08 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.053 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
            "\t Val. Loss: 0.504 |  Val. f1: 0.76 |  Val. precision: 0.77 | Val. recall: 0.76\n",
            "Epoch: 09 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.042 | Train f1: 0.97 | Train precision: 0.97 | Train recall: 0.97\n",
            "\t Val. Loss: 0.537 |  Val. f1: 0.75 |  Val. precision: 0.75 | Val. recall: 0.75\n",
            "Epoch: 10 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.037 | Train f1: 0.97 | Train precision: 0.97 | Train recall: 0.97\n",
            "\t Val. Loss: 0.587 |  Val. f1: 0.75 |  Val. precision: 0.75 | Val. recall: 0.76\n",
            "Epoch: 11 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.031 | Train f1: 0.98 | Train precision: 0.98 | Train recall: 0.98\n",
            "\t Val. Loss: 0.586 |  Val. f1: 0.75 |  Val. precision: 0.77 | Val. recall: 0.74\n",
            "Epoch: 12 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.028 | Train f1: 0.98 | Train precision: 0.98 | Train recall: 0.98\n",
            "\t Val. Loss: 0.588 |  Val. f1: 0.75 |  Val. precision: 0.77 | Val. recall: 0.73\n",
            "Epoch: 13 | Epoch Time: 0m 11s\n",
            "\tTrain Loss: 0.024 | Train f1: 0.98 | Train precision: 0.98 | Train recall: 0.98\n",
            "\t Val. Loss: 0.658 |  Val. f1: 0.74 |  Val. precision: 0.74 | Val. recall: 0.75\n",
            "\n",
            "Best Model:\n",
            "Val. Loss: 0.347 |  Val. f1: 0.76 | Val. precision: 0.79 | Val. recall: 0.74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####LSTM - bi - variando capas"
      ],
      "metadata": {
        "id": "dHv_PybPUhlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 300  # dimensión de los embeddings. \n",
        "HIDDEN_DIM = 256  # dimensión de la capas LSTM \n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
        "\n",
        "N_LAYERS = 5  # número de capas.\n",
        "DROPOUT = 0.3  \n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestros modelos.\n",
        "lstm_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "lstm_model_name = 'LSTMbidDIM2'  # nombre que tendrá el modelo guardado..."
      ],
      "metadata": {
        "id": "N6b33OQ5UhlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = lstm_model\n",
        "model_name = lstm_model_name\n",
        "criterion = baseline_criterion\n",
        "\n",
        "#inicializamos los pesos\n",
        "model.apply(init_weights)\n",
        "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
        "\n",
        "#se determina el optimizador\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "earlyStop_model(model, train_iterator, valid_iterator, optimizer, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxAI4-aSUhlQ",
        "outputId": "81ee901a-e608-4f55-8e81-10eca8c8e866"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El modelo actual tiene 12,734,080 parámetros entrenables.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 21s\n",
            "\tTrain Loss: 0.828 | Train f1: 0.38 | Train precision: 0.57 | Train recall: 0.31\n",
            "\t Val. Loss: 0.551 |  Val. f1: 0.62 |  Val. precision: 0.68 | Val. recall: 0.57\n",
            "Epoch: 02 | Epoch Time: 0m 21s\n",
            "\tTrain Loss: 0.402 | Train f1: 0.72 | Train precision: 0.77 | Train recall: 0.67\n",
            "\t Val. Loss: 0.409 |  Val. f1: 0.72 |  Val. precision: 0.75 | Val. recall: 0.69\n",
            "Epoch: 03 | Epoch Time: 0m 21s\n",
            "\tTrain Loss: 0.251 | Train f1: 0.82 | Train precision: 0.83 | Train recall: 0.81\n",
            "\t Val. Loss: 0.371 |  Val. f1: 0.74 |  Val. precision: 0.74 | Val. recall: 0.74\n",
            "Epoch: 04 | Epoch Time: 0m 21s\n",
            "\tTrain Loss: 0.178 | Train f1: 0.87 | Train precision: 0.87 | Train recall: 0.87\n",
            "\t Val. Loss: 0.377 |  Val. f1: 0.76 |  Val. precision: 0.76 | Val. recall: 0.76\n",
            "Epoch: 05 | Epoch Time: 0m 21s\n",
            "\tTrain Loss: 0.134 | Train f1: 0.90 | Train precision: 0.90 | Train recall: 0.90\n",
            "\t Val. Loss: 0.408 |  Val. f1: 0.75 |  Val. precision: 0.76 | Val. recall: 0.76\n",
            "Epoch: 06 | Epoch Time: 0m 21s\n",
            "\tTrain Loss: 0.106 | Train f1: 0.92 | Train precision: 0.92 | Train recall: 0.92\n",
            "\t Val. Loss: 0.410 |  Val. f1: 0.76 |  Val. precision: 0.76 | Val. recall: 0.76\n",
            "Epoch: 07 | Epoch Time: 0m 21s\n",
            "\tTrain Loss: 0.088 | Train f1: 0.93 | Train precision: 0.93 | Train recall: 0.94\n",
            "\t Val. Loss: 0.449 |  Val. f1: 0.76 |  Val. precision: 0.78 | Val. recall: 0.75\n",
            "Epoch: 08 | Epoch Time: 0m 21s\n",
            "\tTrain Loss: 0.070 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
            "\t Val. Loss: 0.491 |  Val. f1: 0.76 |  Val. precision: 0.78 | Val. recall: 0.75\n",
            "Epoch: 09 | Epoch Time: 0m 21s\n",
            "\tTrain Loss: 0.060 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.96\n",
            "\t Val. Loss: 0.511 |  Val. f1: 0.76 |  Val. precision: 0.77 | Val. recall: 0.75\n",
            "Epoch: 10 | Epoch Time: 0m 21s\n",
            "\tTrain Loss: 0.056 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
            "\t Val. Loss: 0.536 |  Val. f1: 0.76 |  Val. precision: 0.76 | Val. recall: 0.77\n",
            "Epoch: 11 | Epoch Time: 0m 21s\n",
            "\tTrain Loss: 0.048 | Train f1: 0.97 | Train precision: 0.97 | Train recall: 0.97\n",
            "\t Val. Loss: 0.543 |  Val. f1: 0.76 |  Val. precision: 0.77 | Val. recall: 0.75\n",
            "Epoch: 12 | Epoch Time: 0m 21s\n",
            "\tTrain Loss: 0.040 | Train f1: 0.97 | Train precision: 0.97 | Train recall: 0.97\n",
            "\t Val. Loss: 0.549 |  Val. f1: 0.76 |  Val. precision: 0.77 | Val. recall: 0.76\n",
            "Epoch: 13 | Epoch Time: 0m 21s\n",
            "\tTrain Loss: 0.032 | Train f1: 0.97 | Train precision: 0.97 | Train recall: 0.98\n",
            "\t Val. Loss: 0.630 |  Val. f1: 0.75 |  Val. precision: 0.76 | Val. recall: 0.75\n",
            "\n",
            "Best Model:\n",
            "Val. Loss: 0.371 |  Val. f1: 0.74 | Val. precision: 0.74 | Val. recall: 0.74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####LSTM - bi - variando dropout"
      ],
      "metadata": {
        "id": "CsapX3y9bAzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 300  # dimensión de los embeddings. \n",
        "HIDDEN_DIM = 256  # dimensión de la capas LSTM \n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
        "\n",
        "N_LAYERS = 3  # número de capas.\n",
        "DROPOUT = 0.5  \n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestros modelos.\n",
        "lstm_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "lstm_model_name = 'LSTMbidDROP'  # nombre que tendrá el modelo guardado..."
      ],
      "metadata": {
        "id": "aIEGlGplbAzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = lstm_model\n",
        "model_name = lstm_model_name\n",
        "criterion = baseline_criterion\n",
        "\n",
        "#inicializamos los pesos\n",
        "model.apply(init_weights)\n",
        "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
        "\n",
        "#se determina el optimizador\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "earlyStop_model(model, train_iterator, valid_iterator, optimizer, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63e1ea40-5989-4b75-c7b2-59761b59bf1e",
        "id": "-wCOmaTcbAzi"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El modelo actual tiene 9,580,160 parámetros entrenables.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.753 | Train f1: 0.45 | Train precision: 0.62 | Train recall: 0.38\n",
            "\t Val. Loss: 0.465 |  Val. f1: 0.65 |  Val. precision: 0.76 | Val. recall: 0.58\n",
            "Epoch: 02 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.394 | Train f1: 0.72 | Train precision: 0.77 | Train recall: 0.68\n",
            "\t Val. Loss: 0.379 |  Val. f1: 0.73 |  Val. precision: 0.77 | Val. recall: 0.70\n",
            "Epoch: 03 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.267 | Train f1: 0.81 | Train precision: 0.82 | Train recall: 0.79\n",
            "\t Val. Loss: 0.368 |  Val. f1: 0.76 |  Val. precision: 0.77 | Val. recall: 0.76\n",
            "Epoch: 04 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.196 | Train f1: 0.86 | Train precision: 0.86 | Train recall: 0.85\n",
            "\t Val. Loss: 0.349 |  Val. f1: 0.77 |  Val. precision: 0.78 | Val. recall: 0.76\n",
            "Epoch: 05 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.153 | Train f1: 0.89 | Train precision: 0.89 | Train recall: 0.89\n",
            "\t Val. Loss: 0.373 |  Val. f1: 0.78 |  Val. precision: 0.79 | Val. recall: 0.77\n",
            "Epoch: 06 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.124 | Train f1: 0.91 | Train precision: 0.91 | Train recall: 0.91\n",
            "\t Val. Loss: 0.373 |  Val. f1: 0.77 |  Val. precision: 0.78 | Val. recall: 0.76\n",
            "Epoch: 07 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.101 | Train f1: 0.92 | Train precision: 0.92 | Train recall: 0.92\n",
            "\t Val. Loss: 0.424 |  Val. f1: 0.76 |  Val. precision: 0.77 | Val. recall: 0.76\n",
            "Epoch: 08 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.083 | Train f1: 0.94 | Train precision: 0.94 | Train recall: 0.94\n",
            "\t Val. Loss: 0.429 |  Val. f1: 0.76 |  Val. precision: 0.79 | Val. recall: 0.74\n",
            "Epoch: 09 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.072 | Train f1: 0.94 | Train precision: 0.94 | Train recall: 0.95\n",
            "\t Val. Loss: 0.481 |  Val. f1: 0.76 |  Val. precision: 0.77 | Val. recall: 0.75\n",
            "Epoch: 10 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.067 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
            "\t Val. Loss: 0.443 |  Val. f1: 0.77 |  Val. precision: 0.78 | Val. recall: 0.76\n",
            "Epoch: 11 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.055 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
            "\t Val. Loss: 0.519 |  Val. f1: 0.77 |  Val. precision: 0.79 | Val. recall: 0.75\n",
            "Epoch: 12 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.049 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
            "\t Val. Loss: 0.510 |  Val. f1: 0.77 |  Val. precision: 0.79 | Val. recall: 0.75\n",
            "Epoch: 13 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.045 | Train f1: 0.97 | Train precision: 0.97 | Train recall: 0.97\n",
            "\t Val. Loss: 0.531 |  Val. f1: 0.77 |  Val. precision: 0.78 | Val. recall: 0.75\n",
            "Epoch: 14 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.041 | Train f1: 0.97 | Train precision: 0.97 | Train recall: 0.97\n",
            "\t Val. Loss: 0.586 |  Val. f1: 0.76 |  Val. precision: 0.77 | Val. recall: 0.75\n",
            "\n",
            "Best Model:\n",
            "Val. Loss: 0.349 |  Val. f1: 0.77 | Val. precision: 0.78 | Val. recall: 0.76\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####LSTM - bi - variando capas y dropout"
      ],
      "metadata": {
        "id": "lUw5VLsEYl7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 300  # dimensión de los embeddings. \n",
        "HIDDEN_DIM = 256  # dimensión de la capas LSTM \n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
        "\n",
        "N_LAYERS = 5  # número de capas.\n",
        "DROPOUT = 0.5 \n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestros modelos.\n",
        "lstm_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "lstm_model_name = 'LSTMbidCAPASDROP'  # nombre que tendrá el modelo guardado..."
      ],
      "metadata": {
        "id": "Bm0QCjqAYl8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = lstm_model\n",
        "model_name = lstm_model_name\n",
        "criterion = baseline_criterion\n",
        "\n",
        "#inicializamos los pesos\n",
        "model.apply(init_weights)\n",
        "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
        "\n",
        "#se determina el optimizador\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "earlyStop_model(model, train_iterator, valid_iterator, optimizer, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98d519d7-3d3c-469b-8a16-5c301a9dbea0",
        "id": "g12wBPjCYl8B"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El modelo actual tiene 12,734,080 parámetros entrenables.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 21s\n",
            "\tTrain Loss: 0.947 | Train f1: 0.29 | Train precision: 0.49 | Train recall: 0.23\n",
            "\t Val. Loss: 0.650 |  Val. f1: 0.57 |  Val. precision: 0.73 | Val. recall: 0.47\n",
            "Epoch: 02 | Epoch Time: 0m 21s\n",
            "\tTrain Loss: 0.514 | Train f1: 0.65 | Train precision: 0.76 | Train recall: 0.58\n",
            "\t Val. Loss: 0.421 |  Val. f1: 0.69 |  Val. precision: 0.80 | Val. recall: 0.62\n",
            "Epoch: 03 | Epoch Time: 0m 21s\n",
            "\tTrain Loss: 0.336 | Train f1: 0.77 | Train precision: 0.80 | Train recall: 0.74\n",
            "\t Val. Loss: 0.375 |  Val. f1: 0.73 |  Val. precision: 0.75 | Val. recall: 0.71\n",
            "Epoch: 04 | Epoch Time: 0m 21s\n",
            "\tTrain Loss: 0.249 | Train f1: 0.82 | Train precision: 0.83 | Train recall: 0.81\n",
            "\t Val. Loss: 0.394 |  Val. f1: 0.73 |  Val. precision: 0.70 | Val. recall: 0.76\n",
            "Epoch: 05 | Epoch Time: 0m 21s\n",
            "\tTrain Loss: 0.197 | Train f1: 0.86 | Train precision: 0.86 | Train recall: 0.85\n",
            "\t Val. Loss: 0.392 |  Val. f1: 0.75 |  Val. precision: 0.75 | Val. recall: 0.76\n",
            "Epoch: 06 | Epoch Time: 0m 21s\n",
            "\tTrain Loss: 0.161 | Train f1: 0.88 | Train precision: 0.88 | Train recall: 0.88\n",
            "\t Val. Loss: 0.380 |  Val. f1: 0.78 |  Val. precision: 0.79 | Val. recall: 0.77\n",
            "Epoch: 07 | Epoch Time: 0m 21s\n",
            "\tTrain Loss: 0.132 | Train f1: 0.90 | Train precision: 0.90 | Train recall: 0.91\n",
            "\t Val. Loss: 0.401 |  Val. f1: 0.77 |  Val. precision: 0.78 | Val. recall: 0.76\n",
            "Epoch: 08 | Epoch Time: 0m 21s\n",
            "\tTrain Loss: 0.115 | Train f1: 0.91 | Train precision: 0.91 | Train recall: 0.92\n",
            "\t Val. Loss: 0.412 |  Val. f1: 0.77 |  Val. precision: 0.78 | Val. recall: 0.77\n",
            "Epoch: 09 | Epoch Time: 0m 21s\n",
            "\tTrain Loss: 0.102 | Train f1: 0.92 | Train precision: 0.92 | Train recall: 0.92\n",
            "\t Val. Loss: 0.460 |  Val. f1: 0.77 |  Val. precision: 0.78 | Val. recall: 0.78\n",
            "Epoch: 10 | Epoch Time: 0m 21s\n",
            "\tTrain Loss: 0.091 | Train f1: 0.93 | Train precision: 0.93 | Train recall: 0.94\n",
            "\t Val. Loss: 0.487 |  Val. f1: 0.77 |  Val. precision: 0.77 | Val. recall: 0.77\n",
            "Epoch: 11 | Epoch Time: 0m 21s\n",
            "\tTrain Loss: 0.078 | Train f1: 0.94 | Train precision: 0.94 | Train recall: 0.94\n",
            "\t Val. Loss: 0.449 |  Val. f1: 0.77 |  Val. precision: 0.77 | Val. recall: 0.78\n",
            "Epoch: 12 | Epoch Time: 0m 21s\n",
            "\tTrain Loss: 0.070 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
            "\t Val. Loss: 0.494 |  Val. f1: 0.77 |  Val. precision: 0.78 | Val. recall: 0.77\n",
            "Epoch: 13 | Epoch Time: 0m 21s\n",
            "\tTrain Loss: 0.062 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.96\n",
            "\t Val. Loss: 0.567 |  Val. f1: 0.75 |  Val. precision: 0.75 | Val. recall: 0.77\n",
            "\n",
            "Best Model:\n",
            "Val. Loss: 0.375 |  Val. f1: 0.73 | Val. precision: 0.75 | Val. recall: 0.71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####LSTM - bi - variando embedding dimension_lstm =128"
      ],
      "metadata": {
        "id": "FMbb4paSZzNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 500  # dimensión de los embeddings. \n",
        "HIDDEN_DIM = 128  # dimensión de la capas LSTM \n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
        "\n",
        "N_LAYERS = 3  # número de capas.\n",
        "DROPOUT = 0.3 \n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestros modelos.\n",
        "lstm_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "lstm_model_name = 'LSTMbidEMBDIM'  # nombre que tendrá el modelo guardado..."
      ],
      "metadata": {
        "id": "qkOYrFMZZzNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = lstm_model\n",
        "model_name = lstm_model_name\n",
        "criterion = baseline_criterion\n",
        "\n",
        "#inicializamos los pesos\n",
        "model.apply(init_weights)\n",
        "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
        "\n",
        "#se determina el optimizador\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "earlyStop_model(model, train_iterator, valid_iterator, optimizer, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eae816ca-f4ce-4d94-9d5e-4e8ee746e498",
        "id": "4MH0ayCtZzNz"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El modelo actual tiene 10,234,232 parámetros entrenables.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.660 | Train f1: 0.52 | Train precision: 0.64 | Train recall: 0.45\n",
            "\t Val. Loss: 0.395 |  Val. f1: 0.70 |  Val. precision: 0.74 | Val. recall: 0.66\n",
            "Epoch: 02 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.297 | Train f1: 0.79 | Train precision: 0.82 | Train recall: 0.77\n",
            "\t Val. Loss: 0.327 |  Val. f1: 0.77 |  Val. precision: 0.79 | Val. recall: 0.75\n",
            "Epoch: 03 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.178 | Train f1: 0.87 | Train precision: 0.88 | Train recall: 0.87\n",
            "\t Val. Loss: 0.356 |  Val. f1: 0.77 |  Val. precision: 0.80 | Val. recall: 0.75\n",
            "Epoch: 04 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.124 | Train f1: 0.91 | Train precision: 0.91 | Train recall: 0.91\n",
            "\t Val. Loss: 0.378 |  Val. f1: 0.77 |  Val. precision: 0.78 | Val. recall: 0.76\n",
            "Epoch: 05 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.089 | Train f1: 0.94 | Train precision: 0.93 | Train recall: 0.94\n",
            "\t Val. Loss: 0.426 |  Val. f1: 0.75 |  Val. precision: 0.74 | Val. recall: 0.77\n",
            "Epoch: 06 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.070 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
            "\t Val. Loss: 0.423 |  Val. f1: 0.77 |  Val. precision: 0.78 | Val. recall: 0.76\n",
            "Epoch: 07 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.056 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
            "\t Val. Loss: 0.469 |  Val. f1: 0.75 |  Val. precision: 0.76 | Val. recall: 0.75\n",
            "Epoch: 08 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.042 | Train f1: 0.97 | Train precision: 0.97 | Train recall: 0.97\n",
            "\t Val. Loss: 0.550 |  Val. f1: 0.75 |  Val. precision: 0.75 | Val. recall: 0.75\n",
            "Epoch: 09 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.033 | Train f1: 0.98 | Train precision: 0.98 | Train recall: 0.98\n",
            "\t Val. Loss: 0.569 |  Val. f1: 0.76 |  Val. precision: 0.77 | Val. recall: 0.76\n",
            "Epoch: 10 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.028 | Train f1: 0.98 | Train precision: 0.98 | Train recall: 0.98\n",
            "\t Val. Loss: 0.613 |  Val. f1: 0.76 |  Val. precision: 0.78 | Val. recall: 0.75\n",
            "Epoch: 11 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.027 | Train f1: 0.98 | Train precision: 0.98 | Train recall: 0.98\n",
            "\t Val. Loss: 0.605 |  Val. f1: 0.75 |  Val. precision: 0.77 | Val. recall: 0.74\n",
            "Epoch: 12 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.025 | Train f1: 0.98 | Train precision: 0.98 | Train recall: 0.98\n",
            "\t Val. Loss: 0.653 |  Val. f1: 0.74 |  Val. precision: 0.77 | Val. recall: 0.73\n",
            "\n",
            "Best Model:\n",
            "Val. Loss: 0.327 |  Val. f1: 0.77 | Val. precision: 0.79 | Val. recall: 0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####LSTM - bi - variando embedding dimension_lstm =128 y dropout"
      ],
      "metadata": {
        "id": "GIXny1_Yb38x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 500  # dimensión de los embeddings. \n",
        "HIDDEN_DIM = 128  # dimensión de la capas LSTM \n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
        "\n",
        "N_LAYERS = 3  # número de capas.\n",
        "DROPOUT = 0.5 \n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestros modelos.\n",
        "lstm_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "lstm_model_name = 'LSTMbidEMBDIMDROP'  # nombre que tendrá el modelo guardado..."
      ],
      "metadata": {
        "id": "zWj7W0qrb38y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = lstm_model\n",
        "model_name = lstm_model_name\n",
        "criterion = baseline_criterion\n",
        "\n",
        "#inicializamos los pesos\n",
        "model.apply(init_weights)\n",
        "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
        "\n",
        "#se determina el optimizador\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "earlyStop_model(model, train_iterator, valid_iterator, optimizer, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbe31a0c-e211-4a6b-e54d-7b625c791195",
        "id": "tGq9MVGsb38z"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El modelo actual tiene 10,234,232 parámetros entrenables.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.743 | Train f1: 0.47 | Train precision: 0.62 | Train recall: 0.40\n",
            "\t Val. Loss: 0.450 |  Val. f1: 0.67 |  Val. precision: 0.75 | Val. recall: 0.61\n",
            "Epoch: 02 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.373 | Train f1: 0.74 | Train precision: 0.78 | Train recall: 0.71\n",
            "\t Val. Loss: 0.392 |  Val. f1: 0.73 |  Val. precision: 0.78 | Val. recall: 0.70\n",
            "Epoch: 03 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.248 | Train f1: 0.82 | Train precision: 0.84 | Train recall: 0.81\n",
            "\t Val. Loss: 0.365 |  Val. f1: 0.75 |  Val. precision: 0.80 | Val. recall: 0.72\n",
            "Epoch: 04 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.178 | Train f1: 0.87 | Train precision: 0.88 | Train recall: 0.87\n",
            "\t Val. Loss: 0.364 |  Val. f1: 0.77 |  Val. precision: 0.77 | Val. recall: 0.77\n",
            "Epoch: 05 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.138 | Train f1: 0.90 | Train precision: 0.90 | Train recall: 0.90\n",
            "\t Val. Loss: 0.399 |  Val. f1: 0.76 |  Val. precision: 0.77 | Val. recall: 0.76\n",
            "Epoch: 06 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.112 | Train f1: 0.92 | Train precision: 0.92 | Train recall: 0.92\n",
            "\t Val. Loss: 0.421 |  Val. f1: 0.77 |  Val. precision: 0.78 | Val. recall: 0.76\n",
            "Epoch: 07 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.093 | Train f1: 0.93 | Train precision: 0.93 | Train recall: 0.93\n",
            "\t Val. Loss: 0.456 |  Val. f1: 0.77 |  Val. precision: 0.76 | Val. recall: 0.77\n",
            "Epoch: 08 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.077 | Train f1: 0.94 | Train precision: 0.94 | Train recall: 0.95\n",
            "\t Val. Loss: 0.469 |  Val. f1: 0.75 |  Val. precision: 0.77 | Val. recall: 0.75\n",
            "Epoch: 09 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.063 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
            "\t Val. Loss: 0.503 |  Val. f1: 0.76 |  Val. precision: 0.75 | Val. recall: 0.76\n",
            "Epoch: 10 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.056 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
            "\t Val. Loss: 0.522 |  Val. f1: 0.76 |  Val. precision: 0.76 | Val. recall: 0.75\n",
            "Epoch: 11 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.051 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
            "\t Val. Loss: 0.538 |  Val. f1: 0.75 |  Val. precision: 0.76 | Val. recall: 0.74\n",
            "Epoch: 12 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.042 | Train f1: 0.97 | Train precision: 0.97 | Train recall: 0.97\n",
            "\t Val. Loss: 0.617 |  Val. f1: 0.75 |  Val. precision: 0.77 | Val. recall: 0.74\n",
            "Epoch: 13 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.040 | Train f1: 0.97 | Train precision: 0.97 | Train recall: 0.97\n",
            "\t Val. Loss: 0.572 |  Val. f1: 0.74 |  Val. precision: 0.75 | Val. recall: 0.74\n",
            "Epoch: 14 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.035 | Train f1: 0.98 | Train precision: 0.97 | Train recall: 0.98\n",
            "\t Val. Loss: 0.592 |  Val. f1: 0.74 |  Val. precision: 0.73 | Val. recall: 0.74\n",
            "\n",
            "Best Model:\n",
            "Val. Loss: 0.364 |  Val. f1: 0.77 | Val. precision: 0.77 | Val. recall: 0.77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####LSTM - bi - variando embedding version 2, dimension_lstm =128 y dropout"
      ],
      "metadata": {
        "id": "Iezqg22McdN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 800  # dimensión de los embeddings. \n",
        "HIDDEN_DIM = 128  # dimensión de la capas LSTM \n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
        "\n",
        "N_LAYERS = 3  # número de capas.\n",
        "DROPOUT = 0.5 \n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestros modelos.\n",
        "lstm_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "lstm_model_name = 'LSTMbidEMB2DIMDROP'  # nombre que tendrá el modelo guardado..."
      ],
      "metadata": {
        "id": "quu2UK6McdN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = lstm_model\n",
        "model_name = lstm_model_name\n",
        "criterion = baseline_criterion\n",
        "\n",
        "#inicializamos los pesos\n",
        "model.apply(init_weights)\n",
        "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
        "\n",
        "#se determina el optimizador\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "earlyStop_model(model, train_iterator, valid_iterator, optimizer, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffc9faaa-80e3-49f9-eadc-35437b1fcba6",
        "id": "FkyOfHqScdOA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El modelo actual tiene 15,818,732 parámetros entrenables.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.717 | Train f1: 0.49 | Train precision: 0.64 | Train recall: 0.42\n",
            "\t Val. Loss: 0.433 |  Val. f1: 0.69 |  Val. precision: 0.72 | Val. recall: 0.66\n",
            "Epoch: 02 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.349 | Train f1: 0.76 | Train precision: 0.79 | Train recall: 0.73\n",
            "\t Val. Loss: 0.357 |  Val. f1: 0.74 |  Val. precision: 0.75 | Val. recall: 0.73\n",
            "Epoch: 03 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.227 | Train f1: 0.84 | Train precision: 0.85 | Train recall: 0.83\n",
            "\t Val. Loss: 0.352 |  Val. f1: 0.76 |  Val. precision: 0.77 | Val. recall: 0.75\n",
            "Epoch: 04 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.160 | Train f1: 0.89 | Train precision: 0.89 | Train recall: 0.88\n",
            "\t Val. Loss: 0.393 |  Val. f1: 0.76 |  Val. precision: 0.74 | Val. recall: 0.78\n",
            "Epoch: 05 | Epoch Time: 0m 14s\n",
            "\tTrain Loss: 0.121 | Train f1: 0.91 | Train precision: 0.91 | Train recall: 0.91\n",
            "\t Val. Loss: 0.439 |  Val. f1: 0.76 |  Val. precision: 0.75 | Val. recall: 0.77\n",
            "Epoch: 06 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.093 | Train f1: 0.93 | Train precision: 0.93 | Train recall: 0.93\n",
            "\t Val. Loss: 0.470 |  Val. f1: 0.75 |  Val. precision: 0.73 | Val. recall: 0.78\n",
            "Epoch: 07 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.077 | Train f1: 0.94 | Train precision: 0.94 | Train recall: 0.95\n",
            "\t Val. Loss: 0.486 |  Val. f1: 0.76 |  Val. precision: 0.77 | Val. recall: 0.76\n",
            "Epoch: 08 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.063 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
            "\t Val. Loss: 0.498 |  Val. f1: 0.76 |  Val. precision: 0.78 | Val. recall: 0.75\n",
            "Epoch: 09 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.055 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
            "\t Val. Loss: 0.538 |  Val. f1: 0.76 |  Val. precision: 0.77 | Val. recall: 0.75\n",
            "Epoch: 10 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.045 | Train f1: 0.97 | Train precision: 0.97 | Train recall: 0.97\n",
            "\t Val. Loss: 0.560 |  Val. f1: 0.76 |  Val. precision: 0.75 | Val. recall: 0.77\n",
            "Epoch: 11 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.039 | Train f1: 0.97 | Train precision: 0.97 | Train recall: 0.97\n",
            "\t Val. Loss: 0.608 |  Val. f1: 0.75 |  Val. precision: 0.75 | Val. recall: 0.75\n",
            "Epoch: 12 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.034 | Train f1: 0.98 | Train precision: 0.97 | Train recall: 0.98\n",
            "\t Val. Loss: 0.624 |  Val. f1: 0.76 |  Val. precision: 0.75 | Val. recall: 0.76\n",
            "Epoch: 13 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.031 | Train f1: 0.98 | Train precision: 0.98 | Train recall: 0.98\n",
            "\t Val. Loss: 0.663 |  Val. f1: 0.75 |  Val. precision: 0.76 | Val. recall: 0.75\n",
            "\n",
            "Best Model:\n",
            "Val. Loss: 0.352 |  Val. f1: 0.76 | Val. precision: 0.77 | Val. recall: 0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####LSTM - bi - variando embedding version 2, dimension_lstm =128"
      ],
      "metadata": {
        "id": "kHEA_0dndFU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 800  # dimensión de los embeddings. \n",
        "HIDDEN_DIM = 128  # dimensión de la capas LSTM \n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
        "\n",
        "N_LAYERS = 3  # número de capas.\n",
        "DROPOUT = 0.3 \n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestros modelos.\n",
        "lstm_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "lstm_model_name = 'LSTMbidEMB2DIM'  # nombre que tendrá el modelo guardado..."
      ],
      "metadata": {
        "id": "TtfQ5zEidFU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = lstm_model\n",
        "model_name = lstm_model_name\n",
        "criterion = baseline_criterion\n",
        "\n",
        "#inicializamos los pesos\n",
        "model.apply(init_weights)\n",
        "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
        "\n",
        "#se determina el optimizador\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "earlyStop_model(model, train_iterator, valid_iterator, optimizer, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a4bc29b-b158-42e8-b7d8-6b9c6a367941",
        "id": "PjdTueIpdFU4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El modelo actual tiene 15,818,732 parámetros entrenables.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.638 | Train f1: 0.54 | Train precision: 0.66 | Train recall: 0.48\n",
            "\t Val. Loss: 0.406 |  Val. f1: 0.71 |  Val. precision: 0.77 | Val. recall: 0.66\n",
            "Epoch: 02 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.276 | Train f1: 0.80 | Train precision: 0.82 | Train recall: 0.79\n",
            "\t Val. Loss: 0.348 |  Val. f1: 0.76 |  Val. precision: 0.78 | Val. recall: 0.75\n",
            "Epoch: 03 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.160 | Train f1: 0.88 | Train precision: 0.89 | Train recall: 0.88\n",
            "\t Val. Loss: 0.353 |  Val. f1: 0.77 |  Val. precision: 0.78 | Val. recall: 0.77\n",
            "Epoch: 04 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.107 | Train f1: 0.92 | Train precision: 0.92 | Train recall: 0.92\n",
            "\t Val. Loss: 0.412 |  Val. f1: 0.76 |  Val. precision: 0.76 | Val. recall: 0.75\n",
            "Epoch: 05 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.074 | Train f1: 0.94 | Train precision: 0.94 | Train recall: 0.94\n",
            "\t Val. Loss: 0.470 |  Val. f1: 0.75 |  Val. precision: 0.77 | Val. recall: 0.74\n",
            "Epoch: 06 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.058 | Train f1: 0.96 | Train precision: 0.95 | Train recall: 0.96\n",
            "\t Val. Loss: 0.469 |  Val. f1: 0.77 |  Val. precision: 0.77 | Val. recall: 0.76\n",
            "Epoch: 07 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.045 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.97\n",
            "\t Val. Loss: 0.518 |  Val. f1: 0.75 |  Val. precision: 0.79 | Val. recall: 0.73\n",
            "Epoch: 08 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.035 | Train f1: 0.97 | Train precision: 0.97 | Train recall: 0.97\n",
            "\t Val. Loss: 0.598 |  Val. f1: 0.76 |  Val. precision: 0.77 | Val. recall: 0.75\n",
            "Epoch: 09 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.029 | Train f1: 0.98 | Train precision: 0.98 | Train recall: 0.98\n",
            "\t Val. Loss: 0.629 |  Val. f1: 0.75 |  Val. precision: 0.77 | Val. recall: 0.74\n",
            "Epoch: 10 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.025 | Train f1: 0.98 | Train precision: 0.98 | Train recall: 0.98\n",
            "\t Val. Loss: 0.599 |  Val. f1: 0.76 |  Val. precision: 0.77 | Val. recall: 0.76\n",
            "Epoch: 11 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.023 | Train f1: 0.98 | Train precision: 0.98 | Train recall: 0.98\n",
            "\t Val. Loss: 0.648 |  Val. f1: 0.75 |  Val. precision: 0.76 | Val. recall: 0.74\n",
            "Epoch: 12 | Epoch Time: 0m 13s\n",
            "\tTrain Loss: 0.021 | Train f1: 0.98 | Train precision: 0.98 | Train recall: 0.98\n",
            "\t Val. Loss: 0.679 |  Val. f1: 0.75 |  Val. precision: 0.76 | Val. recall: 0.74\n",
            "\n",
            "Best Model:\n",
            "Val. Loss: 0.348 |  Val. f1: 0.76 | Val. precision: 0.78 | Val. recall: 0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####LSTM - bi - variando embedding version 2, dimension_lstm =128, capas=5"
      ],
      "metadata": {
        "id": "q1F9lS2ZeDpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 800  # dimensión de los embeddings. \n",
        "HIDDEN_DIM = 128  # dimensión de la capas LSTM \n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
        "\n",
        "N_LAYERS = 5  # número de capas.\n",
        "DROPOUT = 0.3 \n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestros modelos.\n",
        "lstm_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "lstm_model_name = 'LSTMbidEMB2DIMCAP'  # nombre que tendrá el modelo guardado..."
      ],
      "metadata": {
        "id": "4QGXIOAteDpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = lstm_model\n",
        "model_name = lstm_model_name\n",
        "criterion = baseline_criterion\n",
        "\n",
        "#inicializamos los pesos\n",
        "model.apply(init_weights)\n",
        "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
        "\n",
        "#se determina el optimizador\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "earlyStop_model(model, train_iterator, valid_iterator, optimizer, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2253c342-5347-4d39-80fe-0b221453bfc1",
        "id": "03z9haKmeDpl"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El modelo actual tiene 16,609,260 parámetros entrenables.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 20s\n",
            "\tTrain Loss: 0.726 | Train f1: 0.48 | Train precision: 0.61 | Train recall: 0.41\n",
            "\t Val. Loss: 0.459 |  Val. f1: 0.67 |  Val. precision: 0.75 | Val. recall: 0.61\n",
            "Epoch: 02 | Epoch Time: 0m 18s\n",
            "\tTrain Loss: 0.333 | Train f1: 0.77 | Train precision: 0.80 | Train recall: 0.74\n",
            "\t Val. Loss: 0.376 |  Val. f1: 0.74 |  Val. precision: 0.76 | Val. recall: 0.72\n",
            "Epoch: 03 | Epoch Time: 0m 18s\n",
            "\tTrain Loss: 0.195 | Train f1: 0.86 | Train precision: 0.86 | Train recall: 0.86\n",
            "\t Val. Loss: 0.380 |  Val. f1: 0.76 |  Val. precision: 0.77 | Val. recall: 0.75\n",
            "Epoch: 04 | Epoch Time: 0m 18s\n",
            "\tTrain Loss: 0.135 | Train f1: 0.90 | Train precision: 0.90 | Train recall: 0.90\n",
            "\t Val. Loss: 0.396 |  Val. f1: 0.75 |  Val. precision: 0.75 | Val. recall: 0.76\n",
            "Epoch: 05 | Epoch Time: 0m 18s\n",
            "\tTrain Loss: 0.100 | Train f1: 0.93 | Train precision: 0.93 | Train recall: 0.93\n",
            "\t Val. Loss: 0.444 |  Val. f1: 0.75 |  Val. precision: 0.76 | Val. recall: 0.75\n",
            "Epoch: 06 | Epoch Time: 0m 19s\n",
            "\tTrain Loss: 0.079 | Train f1: 0.94 | Train precision: 0.94 | Train recall: 0.95\n",
            "\t Val. Loss: 0.483 |  Val. f1: 0.75 |  Val. precision: 0.77 | Val. recall: 0.74\n",
            "Epoch: 07 | Epoch Time: 0m 18s\n",
            "\tTrain Loss: 0.063 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.96\n",
            "\t Val. Loss: 0.521 |  Val. f1: 0.75 |  Val. precision: 0.75 | Val. recall: 0.75\n",
            "Epoch: 08 | Epoch Time: 0m 18s\n",
            "\tTrain Loss: 0.049 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.97\n",
            "\t Val. Loss: 0.554 |  Val. f1: 0.74 |  Val. precision: 0.74 | Val. recall: 0.75\n",
            "Epoch: 09 | Epoch Time: 0m 18s\n",
            "\tTrain Loss: 0.040 | Train f1: 0.97 | Train precision: 0.97 | Train recall: 0.97\n",
            "\t Val. Loss: 0.588 |  Val. f1: 0.74 |  Val. precision: 0.75 | Val. recall: 0.74\n",
            "Epoch: 10 | Epoch Time: 0m 18s\n",
            "\tTrain Loss: 0.036 | Train f1: 0.97 | Train precision: 0.97 | Train recall: 0.97\n",
            "\t Val. Loss: 0.599 |  Val. f1: 0.75 |  Val. precision: 0.76 | Val. recall: 0.74\n",
            "Epoch: 11 | Epoch Time: 0m 18s\n",
            "\tTrain Loss: 0.029 | Train f1: 0.98 | Train precision: 0.98 | Train recall: 0.98\n",
            "\t Val. Loss: 0.693 |  Val. f1: 0.75 |  Val. precision: 0.75 | Val. recall: 0.74\n",
            "Epoch: 12 | Epoch Time: 0m 18s\n",
            "\tTrain Loss: 0.027 | Train f1: 0.98 | Train precision: 0.98 | Train recall: 0.98\n",
            "\t Val. Loss: 0.610 |  Val. f1: 0.74 |  Val. precision: 0.76 | Val. recall: 0.73\n",
            "\n",
            "Best Model:\n",
            "Val. Loss: 0.376 |  Val. f1: 0.74 | Val. precision: 0.76 | Val. recall: 0.72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####LSTM - bi - variando embedding version 2, dimension_lstm =128, capas=5, DROPOUT"
      ],
      "metadata": {
        "id": "v_BWgSfieiT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 800  # dimensión de los embeddings. \n",
        "HIDDEN_DIM = 128  # dimensión de la capas LSTM \n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
        "\n",
        "N_LAYERS = 5  # número de capas.\n",
        "DROPOUT = 0.3 \n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Creamos nuestros modelos.\n",
        "lstm_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "lstm_model_name = 'LSTMbidEMB2DIMCAPDROP'  # nombre que tendrá el modelo guardado..."
      ],
      "metadata": {
        "id": "K0h50aaGeiT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = lstm_model\n",
        "model_name = lstm_model_name\n",
        "criterion = baseline_criterion\n",
        "\n",
        "#inicializamos los pesos\n",
        "model.apply(init_weights)\n",
        "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
        "\n",
        "#se determina el optimizador\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "earlyStop_model(model, train_iterator, valid_iterator, optimizer, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "412b5ac2-c1ad-490f-afc2-444b3c306697",
        "id": "HyS8C1jheiT4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El modelo actual tiene 16,609,260 parámetros entrenables.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: <pad> seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 18s\n",
            "\tTrain Loss: 0.709 | Train f1: 0.50 | Train precision: 0.64 | Train recall: 0.43\n",
            "\t Val. Loss: 0.438 |  Val. f1: 0.70 |  Val. precision: 0.74 | Val. recall: 0.66\n",
            "Epoch: 02 | Epoch Time: 0m 18s\n",
            "\tTrain Loss: 0.325 | Train f1: 0.78 | Train precision: 0.80 | Train recall: 0.75\n",
            "\t Val. Loss: 0.354 |  Val. f1: 0.74 |  Val. precision: 0.75 | Val. recall: 0.74\n",
            "Epoch: 03 | Epoch Time: 0m 18s\n",
            "\tTrain Loss: 0.197 | Train f1: 0.86 | Train precision: 0.86 | Train recall: 0.86\n",
            "\t Val. Loss: 0.396 |  Val. f1: 0.76 |  Val. precision: 0.79 | Val. recall: 0.73\n",
            "Epoch: 04 | Epoch Time: 0m 18s\n",
            "\tTrain Loss: 0.136 | Train f1: 0.90 | Train precision: 0.90 | Train recall: 0.90\n",
            "\t Val. Loss: 0.419 |  Val. f1: 0.75 |  Val. precision: 0.75 | Val. recall: 0.75\n",
            "Epoch: 05 | Epoch Time: 0m 19s\n",
            "\tTrain Loss: 0.104 | Train f1: 0.92 | Train precision: 0.92 | Train recall: 0.93\n",
            "\t Val. Loss: 0.469 |  Val. f1: 0.77 |  Val. precision: 0.79 | Val. recall: 0.75\n",
            "Epoch: 06 | Epoch Time: 0m 18s\n",
            "\tTrain Loss: 0.077 | Train f1: 0.94 | Train precision: 0.94 | Train recall: 0.95\n",
            "\t Val. Loss: 0.520 |  Val. f1: 0.75 |  Val. precision: 0.75 | Val. recall: 0.75\n",
            "Epoch: 07 | Epoch Time: 0m 18s\n",
            "\tTrain Loss: 0.061 | Train f1: 0.96 | Train precision: 0.95 | Train recall: 0.96\n",
            "\t Val. Loss: 0.526 |  Val. f1: 0.76 |  Val. precision: 0.77 | Val. recall: 0.74\n",
            "Epoch: 08 | Epoch Time: 0m 18s\n",
            "\tTrain Loss: 0.048 | Train f1: 0.97 | Train precision: 0.96 | Train recall: 0.97\n",
            "\t Val. Loss: 0.551 |  Val. f1: 0.75 |  Val. precision: 0.75 | Val. recall: 0.75\n",
            "Epoch: 09 | Epoch Time: 0m 18s\n",
            "\tTrain Loss: 0.044 | Train f1: 0.97 | Train precision: 0.97 | Train recall: 0.97\n",
            "\t Val. Loss: 0.605 |  Val. f1: 0.75 |  Val. precision: 0.76 | Val. recall: 0.75\n",
            "Epoch: 10 | Epoch Time: 0m 18s\n",
            "\tTrain Loss: 0.035 | Train f1: 0.98 | Train precision: 0.97 | Train recall: 0.98\n",
            "\t Val. Loss: 0.586 |  Val. f1: 0.75 |  Val. precision: 0.73 | Val. recall: 0.76\n",
            "Epoch: 11 | Epoch Time: 0m 18s\n",
            "\tTrain Loss: 0.029 | Train f1: 0.98 | Train precision: 0.98 | Train recall: 0.98\n",
            "\t Val. Loss: 0.638 |  Val. f1: 0.76 |  Val. precision: 0.77 | Val. recall: 0.76\n",
            "Epoch: 12 | Epoch Time: 0m 18s\n",
            "\tTrain Loss: 0.033 | Train f1: 0.98 | Train precision: 0.97 | Train recall: 0.98\n",
            "\t Val. Loss: 0.619 |  Val. f1: 0.75 |  Val. precision: 0.75 | Val. recall: 0.75\n",
            "\n",
            "Best Model:\n",
            "Val. Loss: 0.354 |  Val. f1: 0.74 | Val. precision: 0.75 | Val. recall: 0.74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uF1ysw_Kw6zz"
      },
      "source": [
        "### **Predecir datos para la competencia**\n",
        "\n",
        "Ahora, a partir de los datos de **test** y nuestro modelo entrenado, vamos a predecir las etiquetas que serán evaluadas en la competencia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RBs3UU4wLk3"
      },
      "source": [
        "def predict_labels(model, iterator, criterion, fields=fields):\n",
        "\n",
        "    # Extraemos los vocabularios.\n",
        "    text_field = fields[0][1]\n",
        "    nertags_field = fields[1][1]\n",
        "    tags_vocab = nertags_field.vocab.itos\n",
        "    words_vocab = text_field.vocab.itos\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for batch in iterator:\n",
        "\n",
        "            text_batch = batch.text\n",
        "            text_batch = torch.transpose(text_batch, 0, 1).tolist()\n",
        "\n",
        "            # Predecir los tags de las sentences del batch\n",
        "            predictions_batch = model(batch.text)\n",
        "            predictions_batch = torch.transpose(predictions_batch, 0, 1)\n",
        "\n",
        "            # por cada oración predicha:\n",
        "            for sentence, sentence_prediction in zip(text_batch,\n",
        "                                                     predictions_batch):\n",
        "                for word_idx, word_predictions in zip(sentence,\n",
        "                                                      sentence_prediction):\n",
        "                    # Obtener el indice del tag con la probabilidad mas alta.\n",
        "                    argmax_index = word_predictions.topk(1)[1]\n",
        "\n",
        "                    current_tag = tags_vocab[argmax_index]\n",
        "                    # Obtenemos la palabra\n",
        "                    current_word = words_vocab[word_idx]\n",
        "\n",
        "                    if current_word != '<pad>':\n",
        "                        predictions.append([current_word, current_tag])\n",
        "                predictions.append(['EOS', 'EOS'])\n",
        "\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "predictions = predict_labels(model, test_iterator, criterion)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "metadata": {
        "id": "mQ3Rv2KvUZr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwQp1Ru8Oht8"
      },
      "source": [
        "### **Generar el archivo para la submission**\n",
        "\n",
        "No hay problema si aparecen unk en la salida. Estos no son relevantes para evaluarlos, usamos solo los tags."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPfZkjJGkWyq"
      },
      "source": [
        "import os, shutil\n",
        "\n",
        "if (os.path.isfile('./predictions.zip')):\n",
        "    os.remove('./predictions.zip')\n",
        "\n",
        "if (not os.path.isdir('./predictions')):\n",
        "    os.mkdir('./predictions')\n",
        "\n",
        "else:\n",
        "    # Eliminar predicciones anteriores:\n",
        "    shutil.rmtree('./predictions')\n",
        "    os.mkdir('./predictions')\n",
        "\n",
        "f = open('predictions/predictions.txt', 'w')\n",
        "for i, (word, tag) in enumerate(predictions[:-1]):\n",
        "    if word=='EOS' and tag=='EOS': f.write('\\n')\n",
        "    else: \n",
        "      if i == len(predictions[:-1])-1:\n",
        "        f.write(word + ' ' + tag)\n",
        "      else: f.write(word + ' ' + tag + '\\n')\n",
        "\n",
        "f.close()\n",
        "\n",
        "a = shutil.make_archive('predictions', 'zip', './predictions')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Resultados**\n",
        "\n",
        "|No.| Modelo                       |Epochs|Loss | f1 |precision|recall|\n",
        "|---|------------------------------|------|-----|----|---------|------|\n",
        "| 0 |LSTM                          |  13  |0.381|0.73|  0.73   | 0.72 |\n",
        "| 1 |Elman + earlyStop(ES)         |  20  |0.468|0.69|  0.71   | 0.66 |\n",
        "| 2 |GRU + ES                      |  12  |0.399|0.72|  0.78   | 0.67 |\n",
        "| 3 |LSTM Bidireccional (BI) + ES  |  12  |0.332|0.75|  0.79   | 0.72 |\n",
        "| 4 |Elman BI + ES                 |  10  |0.716|0.52|  0.62   | 0.45 |\n",
        "| 5 |GRU BI + ES                   |  13  |0.366|0.75|  0.78   | 0.73 |\n",
        "| 6 |LSTM BI + ES + EMBEDDING (EMB)|  12  |0.340|0.76|  0.78   | 0.75 |\n",
        "| 7 |LSTM BI + ES + DIM=512 (D1)   |  13  |0.368|0.76|  0.79   | 0.74 |\n",
        "| 8 |LSTM BI + ES + DIM=128 (D2)   |  13  |0.347|0.76|  0.79   | 0.74 |\n",
        "| 9 |LSTM BI + ES + CAPAS=5 (CA)   |  13  |0.371|0.74|  0.74   | 0.74 |\n",
        "| 10|LSTM BI + ES + DROPOUT=0.5(DR)|  14  |0.349|0.77|  0.78   | 0.76 |\n",
        "| 11|LSTM BI + ES + CA + DR        |  13  |0.375|0.73|  0.75   | 0.71 |\n",
        "| 12|LSTM BI + ES + EMB + D2       |  12  |0.327|0.77|  0.79   | 0.75 |\n",
        "| 13|LSTM BI + ES + EMB + D2 + DR  |  14  |0.364|0.77|  0.77   | 0.77 |\n",
        "| 14|LSTM BI + ES + EMB2 + D2 + DR |  13  |0.352|0.76|  0.77   | 0.75 |\n",
        "| 15|LSTM BI + ES + EMB2 + D2      |  12  |0.344|0.78|  0.80   | 0.76 |\n",
        "| 16|LSTM BI + ES + EMB2 + D2 + CA |  14  |0.376|0.74|  0.76   | 0.72 |\n",
        "| 17|LSTM BI + ES + EMB2 + D2 + CA + DR|  12  |0.354|0.74|  0.75   | 0.74 |\n",
        "\n"
      ],
      "metadata": {
        "id": "8xkOtUj1JpEs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observamos de los resultados mostrados previamente, que los modelos bidireccionales son consistentemente mejores que los modelos sin bidireccionalidad. Ahora bien, dentro de estos, el modelo con mejores resultados es la LSTM. Al probar distintas arquitecturas de LSTM vemos que el modelo que obtiene la menor loss es el modelo número 12 que mezcla un cambio en el embedding y un menor numero de hidden_layers. No obstante, el modelo con mejor métrica de desempeño en la predicción es el modelo 15 que incorpora otra versión del cambio de embedding. Ahora bien, la mejora es muy menor para la cantidad de parametros extra que se crean, por lo que se concluye que el mejor modelo creado es el número 12."
      ],
      "metadata": {
        "id": "BiuNZlaJkJ0N"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZEWJXrNaSIf"
      },
      "source": [
        "## **Conclusiones**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAtK7y43V7Z_"
      },
      "source": [
        "Los resultados en el dataset de validación fueron razonablemente buenos. No obstante, se observó mucho overfitting en los modelos planteados. Lo cual se trasujo en un bajo número de epocas de entrenamiento. Respecto a las métricas, se tienen resultados razonables, pero que al extrapolarse al dataset de testeo, fueron inferiores a las esperadas.\n",
        "\n",
        "Para poder mejorar los modelos, se propone en primer lugar, variar el learning rate a valores menores, pues una opcion posible es que las redes estén cayendo dentro de un mínimo local que propicie el overfitting. Pasando a metodologías posibles a aplicar, se tiene el uso de transformers, otros embeddings para generar los inputs de las redes neuronales o realizar el ejercicio de distintas arquitecturas sobre modelo GRU."
      ]
    }
  ]
}